{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WhatLies \u00b6 A library that tries help you to understand. \"What lies in word embeddings?\" Brief Introduction \u00b6 If you prefer a video tutorial before reading the getting started guide watch this; Produced \u00b6 This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa. What it Does \u00b6 This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out. Installation \u00b6 You can install the package via pip; pip install whatlies This will install the base dependencies. Depending on the transformers and language backends that you'll be using you may want to install more. Here's all the possible installation settings you could go for. pip install whatlies[base] pip install whatlies[tfhub] pip install whatlies[transformers] pip install whatlies[ivis] pip install whatlies[opentsne] pip install whatlies[sense2vec] If you want it all you can also install via; pip install whatlies[all] Note that this will install dependencies but it will not install all the language models you might want to visualise. For example, you might still need to manually download spaCy models if you intend to use that backend. Similar Projects \u00b6 There are some projects out there who are working on similar tools and we figured it fair to mention and compare them here. Julia Bazi\u0144ska & Piotr Migdal Web App \u00b6 The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends. Tensorflow Projector \u00b6 From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it. Parallax \u00b6 From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental. Local Development \u00b6 If you want to develop locally you can start by running this command after cloning. make develop","title":"Home"},{"location":"#whatlies","text":"A library that tries help you to understand. \"What lies in word embeddings?\"","title":"WhatLies"},{"location":"#brief-introduction","text":"If you prefer a video tutorial before reading the getting started guide watch this;","title":"Brief Introduction"},{"location":"#produced","text":"This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa.","title":"Produced"},{"location":"#what-it-does","text":"This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out.","title":"What it Does"},{"location":"#installation","text":"You can install the package via pip; pip install whatlies This will install the base dependencies. Depending on the transformers and language backends that you'll be using you may want to install more. Here's all the possible installation settings you could go for. pip install whatlies[base] pip install whatlies[tfhub] pip install whatlies[transformers] pip install whatlies[ivis] pip install whatlies[opentsne] pip install whatlies[sense2vec] If you want it all you can also install via; pip install whatlies[all] Note that this will install dependencies but it will not install all the language models you might want to visualise. For example, you might still need to manually download spaCy models if you intend to use that backend.","title":"Installation"},{"location":"#similar-projects","text":"There are some projects out there who are working on similar tools and we figured it fair to mention and compare them here.","title":"Similar Projects"},{"location":"#julia-bazinska-piotr-migdal-web-app","text":"The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends.","title":"Julia Bazi\u0144ska &amp; Piotr Migdal Web App"},{"location":"#tensorflow-projector","text":"From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it.","title":"Tensorflow Projector"},{"location":"#parallax","text":"From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental.","title":"Parallax"},{"location":"#local-development","text":"If you want to develop locally you can start by running this command after cloning. make develop","title":"Local Development"},{"location":"faq/","text":"F.A.Q. \u00b6 Plotting \u00b6 How do I save an interactive chart? \u00b6 The interactive charts that our library produces are made with altair . These charts use javascript for the interactivity and they are based on vega . You can represent the entire chart (including the data) as a json object. This means that you can always save a visluatisation as an html page or as a json file. from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.to_html(\"plot.html\") p.to_json(\"plot.json\") A tutorial on how this works exactly can be found here . How do I save an interactive chart for publication? \u00b6 You can also choose to save an interactive chart as an svg/png/pdf if you're interested in using an altair visualisation in a publication. More details are listed on their documentation page in short you'll need to install the altair_saver package for this functionality. To get this code to work you may need to install some node dependencies though. To install them locally in your project run; npm install vega-lite vega-cli canvas Once these are all installed, the following code snippet will work; from whatlies.language import SpacyLanguage from altair_saver import save words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') save(p, \"chart.png\") This saves the following chart on disk; How do I change the title/size of the interactive chart? \u00b6 The interactive charts are Altair charts and that means that you could do something like this: from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.properties(title=\"spaCy\", height=200, width=200) One common feature is that you might set the width to the container size in order to achieve 100% width. p.properties(title=\"spaCy\", height=200, width=\"container\") Languages \u00b6 How do I access nearest tokens from a language model? \u00b6 This depends on the language model, please check the docs, but most language models will have a score_similar method attached. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") lang.score_similar(\"king\") This code snippet will return yield; [(Emb[king], 1.1102230246251565e-16), (Emb[\u2581king], 0.23501371664985227), (Emb[iv], 0.33016763827104456), (Emb[\u2581throne], 0.3366865106345296), (Emb[iii], 0.33745878416967634), (Emb[lord], 0.37137511153954517), (Emb[\u2581prince], 0.3806569732193965), (Emb[\u2581duke], 0.3889479082730939), (Emb[son], 0.3892961048683081), (Emb[ivals], 0.3904733871620414)] In this case you'll see subword embeddings being return because that is what this language model uses internally. Language models using spaCy would use full tokens. How do I access nearest tokens from a language model using an embedding? \u00b6 You can pass this method a string, but also an embedding object. Ths can contain a custom vector but you can also construct an embedding via operations. This makes the API a lot more flexible. For example, we can construct this embedding; from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] # Emb[((king - man) + woman)] And use this embedding in our language model to retreive similar items. lang.score_similar(kmw, n=7) This yields. [(Emb[king], 0.2620711370759745), (Emb[mother], 0.36575381150291), (Emb[father], 0.39737356910585997), (Emb[\u2581queen], 0.43554929266740294), (Emb[anne], 0.4583618203004909), (Emb[\u2581throne], 0.47000919280368636), (Emb[mary], 0.4771824121946612)] Note that in general we're using cosine distance here but you can also pass the .score_similar method a metric so select other metrics that are compatible with scikit-learn. How do I retreive an embedding set from language model using similar tokens? \u00b6 You can use the same flow we used in the previous two questions to generate an embedding set that can be used for plotting. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] emb_king = lang.embset_similar(kmw, n=20) Compatibility \u00b6 This project depends on a lot of backends so there's a risk of breaking changes whenever we upgrade to a new version of a backend. The goal of this table is to list keep track of compatible versions. whatlies spaCy tensorflow 0.4.5 2.2.4 2.3.0 0.5.0 2.3.2 2.3.0","title":"FAQ"},{"location":"faq/#faq","text":"","title":"F.A.Q."},{"location":"faq/#plotting","text":"","title":"Plotting"},{"location":"faq/#how-do-i-save-an-interactive-chart","text":"The interactive charts that our library produces are made with altair . These charts use javascript for the interactivity and they are based on vega . You can represent the entire chart (including the data) as a json object. This means that you can always save a visluatisation as an html page or as a json file. from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.to_html(\"plot.html\") p.to_json(\"plot.json\") A tutorial on how this works exactly can be found here .","title":"How do I save an interactive chart?"},{"location":"faq/#how-do-i-save-an-interactive-chart-for-publication","text":"You can also choose to save an interactive chart as an svg/png/pdf if you're interested in using an altair visualisation in a publication. More details are listed on their documentation page in short you'll need to install the altair_saver package for this functionality. To get this code to work you may need to install some node dependencies though. To install them locally in your project run; npm install vega-lite vega-cli canvas Once these are all installed, the following code snippet will work; from whatlies.language import SpacyLanguage from altair_saver import save words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') save(p, \"chart.png\") This saves the following chart on disk;","title":"How do I save an interactive chart for publication?"},{"location":"faq/#how-do-i-change-the-titlesize-of-the-interactive-chart","text":"The interactive charts are Altair charts and that means that you could do something like this: from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.properties(title=\"spaCy\", height=200, width=200) One common feature is that you might set the width to the container size in order to achieve 100% width. p.properties(title=\"spaCy\", height=200, width=\"container\")","title":"How do I change the title/size of the interactive chart?"},{"location":"faq/#languages","text":"","title":"Languages"},{"location":"faq/#how-do-i-access-nearest-tokens-from-a-language-model","text":"This depends on the language model, please check the docs, but most language models will have a score_similar method attached. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") lang.score_similar(\"king\") This code snippet will return yield; [(Emb[king], 1.1102230246251565e-16), (Emb[\u2581king], 0.23501371664985227), (Emb[iv], 0.33016763827104456), (Emb[\u2581throne], 0.3366865106345296), (Emb[iii], 0.33745878416967634), (Emb[lord], 0.37137511153954517), (Emb[\u2581prince], 0.3806569732193965), (Emb[\u2581duke], 0.3889479082730939), (Emb[son], 0.3892961048683081), (Emb[ivals], 0.3904733871620414)] In this case you'll see subword embeddings being return because that is what this language model uses internally. Language models using spaCy would use full tokens.","title":"How do I access nearest tokens from a language model?"},{"location":"faq/#how-do-i-access-nearest-tokens-from-a-language-model-using-an-embedding","text":"You can pass this method a string, but also an embedding object. Ths can contain a custom vector but you can also construct an embedding via operations. This makes the API a lot more flexible. For example, we can construct this embedding; from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] # Emb[((king - man) + woman)] And use this embedding in our language model to retreive similar items. lang.score_similar(kmw, n=7) This yields. [(Emb[king], 0.2620711370759745), (Emb[mother], 0.36575381150291), (Emb[father], 0.39737356910585997), (Emb[\u2581queen], 0.43554929266740294), (Emb[anne], 0.4583618203004909), (Emb[\u2581throne], 0.47000919280368636), (Emb[mary], 0.4771824121946612)] Note that in general we're using cosine distance here but you can also pass the .score_similar method a metric so select other metrics that are compatible with scikit-learn.","title":"How do I access nearest tokens from a language model using an embedding?"},{"location":"faq/#how-do-i-retreive-an-embedding-set-from-language-model-using-similar-tokens","text":"You can use the same flow we used in the previous two questions to generate an embedding set that can be used for plotting. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] emb_king = lang.embset_similar(kmw, n=20)","title":"How do I retreive an embedding set from language model using similar tokens?"},{"location":"faq/#compatibility","text":"This project depends on a lot of backends so there's a risk of breaking changes whenever we upgrade to a new version of a backend. The goal of this table is to list keep track of compatible versions. whatlies spaCy tensorflow 0.4.5 2.2.4 2.3.0 0.5.0 2.3.2 2.3.0","title":"Compatibility"},{"location":"releases/","text":"v0.5.2 Fixed the ConveRTLanguage backend. The original source changed their download url. v0.5.2 Added tests for matplotlib and altair . Added plot_3d , allowing you to make some 3d visualisations. Added assign as a nicer alternative for add_property . Added a citation to an research paper on this library. Removed the \"helper vectors\" from our transformers. v0.5.1 Added a guide on debiasing on the docs. You can now specify the plot axes and title. We've added sensible default values to \"plot_interactive\". We now assume that you want to plot the first two elements of a vector. # Before emb.transform(Pca(2)).plot_interactive('pca_0', 'pca_1') # After emb.transform(Pca(2)).plot_interactive() v0.5.0 Added some robustness tot he matplotlib based arrow and scatter charts. Started deprecating the plot_correlation method in favor of the new plot_distance and plot_similarity methods. v0.4.7 Fixed bugs relating to conditional imports. Added a new pipe method. v0.4.6 Fixed bugs to become spaCy 2.3 compatible. Added scikit-learn pipeline compatibility for all models. v0.4.5 Created support for tfhub and huggingface backends. Added the ivis transformer. v0.4.4 Added support for ConveRT Embeddings. v0.4.3 Added support for similarity retreival for CountVectorLang Added more methods for Embedding objects: distance , norm v0.4.2 Added support the gensim language backend. v0.4.1 Added support for TSNE v0.4.0 Many small updates to improve documentation Fixed many small bugs for the BytePairLanguage","title":"Releases"},{"location":"roadmap/","text":"There's a few things that would be nice to have. Feel free to start a discussion on these topics in github. Curated Word Lists If people want to investigate bias in word embeddings then it would help if they had word-lists at the ready. But you can also imagine that it would be nice to have a word list of words that have multiple meanings. It'd be especially nice if we can support these word lists in many languages. Multiple Plot Metrics At the moment we only project onto axes to get x/y coordinates. It might make sense to show the cosine distance to these axes instead. And if we're allowing cosine distance ... we might allow for flexible distance metrics in general. Table Summaries We've got a focus on charts now, but one imagines that calculating tables with summary statistics is also relevant. Languages It might be nice if we could help the user automatically download embedding files for supported backends. Testing it would be nice to have a good way of testing the charts it would be nice to be able to test multiple models without having to download gigabytes into github actions","title":"Roadmap"},{"location":"api/embedding/","text":"whatlies.embedding.Embedding \u00b6 This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar foo - bar + bar ndim : (property, readonly) \u00b6 Return the dimension of embedding vector. norm : (property, readonly) \u00b6 Gives the norm of the vector of the embedding __add__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __add__ ( self , other ) -> \"Embedding\" : \"\"\" Add two embeddings together. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo + bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } + { other . name } )\" copied . vector = self . vector + other . vector return copied Add two embeddings together. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo + bar __gt__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def __gt__ ( self , other ): \"\"\" Measures the size of one embedding to another one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo > bar ``` \"\"\" return ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) Measures the size of one embedding to another one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo > bar __neg__ ( self ) \u00b6 Show source code in whatlies/embedding.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __neg__ ( self ): \"\"\" Negate an embedding. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) assert (- foo).vector == - foo.vector ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"(- { self . name } )\" copied . vector = - self . vector return copied Negate an embedding. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) assert ( - foo ) . vector == - foo . vector __or__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def __or__ ( self , other ): \"\"\" Makes one embedding orthogonal to the other one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } | { other . name } )\" copied . vector = self . vector - ( self >> other ) . vector return copied Makes one embedding orthogonal to the other one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __rshift__ ( self , other ): \"\"\" Maps an embedding unto another one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo >> bar ``` \"\"\" copied = deepcopy ( self ) new_vec = ( ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) * other . vector ) copied . name = f \"( { self . name } >> { other . name } )\" copied . vector = new_vec return copied Maps an embedding unto another one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo >> bar __sub__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __sub__ ( self , other ): \"\"\" Subtract two embeddings. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo - bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } - { other . name } )\" copied . vector = self . vector - other . vector return copied Subtract two embeddings. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo - bar copy ( self ) \u00b6 Show source code in whatlies/embedding.py 50 51 52 53 54 def copy ( self ): \"\"\" Returns a deepcopy of the embdding. \"\"\" return deepcopy ( self ) Returns a deepcopy of the embdding. distance ( self , other , metric = 'cosine' ) \u00b6 Show source code in whatlies/embedding.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def distance ( self , other , metric : str = \"cosine\" ): \"\"\" Calculates the vector distance between two embeddings. Arguments: other: the other embedding you're comparing against metric: the distance metric to use, the list of valid options can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 0.5]) foo.distance(bar) foo.distance(bar, metric=\"euclidean\") foo.distance(bar, metric=\"cosine\") ``` \"\"\" return pairwise_distances ([ self . vector ], [ other . vector ], metric = metric )[ 0 ][ 0 ] Calculates the vector distance between two embeddings. Parameters Name Type Description Default other the other embedding you're comparing against required metric str the distance metric to use, the list of valid options can be found here 'cosine' Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 0.5 ]) foo . distance ( bar ) foo . distance ( bar , metric = \"euclidean\" ) foo . distance ( bar , metric = \"cosine\" ) plot ( self , kind = 'arrow' , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , color = None , show_ops = False , annot = True , axis_option = None ) \u00b6 Show source code in whatlies/embedding.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , \"Embedding\" ] = 0 , y_axis : Union [ int , \"Embedding\" ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric x_val , x_lab = self . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = self . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label emb_plot = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( emb_plot , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"Embedding"},{"location":"api/embedding/#whatliesembeddingembedding","text":"This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar foo - bar + bar","title":"whatlies.embedding.Embedding"},{"location":"api/embedding/#whatlies.embedding.Embedding.ndim","text":"Return the dimension of embedding vector.","title":"ndim"},{"location":"api/embedding/#whatlies.embedding.Embedding.norm","text":"Gives the norm of the vector of the embedding","title":"norm"},{"location":"api/embedding/#whatlies.embedding.Embedding.copy","text":"Show source code in whatlies/embedding.py 50 51 52 53 54 def copy ( self ): \"\"\" Returns a deepcopy of the embdding. \"\"\" return deepcopy ( self ) Returns a deepcopy of the embdding.","title":"copy()"},{"location":"api/embedding/#whatlies.embedding.Embedding.distance","text":"Show source code in whatlies/embedding.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def distance ( self , other , metric : str = \"cosine\" ): \"\"\" Calculates the vector distance between two embeddings. Arguments: other: the other embedding you're comparing against metric: the distance metric to use, the list of valid options can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 0.5]) foo.distance(bar) foo.distance(bar, metric=\"euclidean\") foo.distance(bar, metric=\"cosine\") ``` \"\"\" return pairwise_distances ([ self . vector ], [ other . vector ], metric = metric )[ 0 ][ 0 ] Calculates the vector distance between two embeddings. Parameters Name Type Description Default other the other embedding you're comparing against required metric str the distance metric to use, the list of valid options can be found here 'cosine' Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 0.5 ]) foo . distance ( bar ) foo . distance ( bar , metric = \"euclidean\" ) foo . distance ( bar , metric = \"cosine\" )","title":"distance()"},{"location":"api/embedding/#whatlies.embedding.Embedding.plot","text":"Show source code in whatlies/embedding.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , \"Embedding\" ] = 0 , y_axis : Union [ int , \"Embedding\" ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric x_val , x_lab = self . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = self . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label emb_plot = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( emb_plot , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"plot()"},{"location":"api/embeddingset/","text":"whatlies.embeddingset.EmbeddingSet \u00b6 This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of Embedding , or a single dictionary containing name: Embedding pairs name : custom name of embeddingset Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) emb = EmbeddingSet ( foo , bar ) emb = EmbeddingSet ({ 'foo' : foo , 'bar' : bar ) ndim : (property, readonly) \u00b6 Return dimension of embedding vectors in embeddingset. __add__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def __add__ ( self , other ): \"\"\" Adds an embedding to each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb + buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb + other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } + { other . name } )\" ) Adds an embedding to each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb + buz ) . plot ( kind = \"arrow\" ) __contains__ ( self , item ) \u00b6 Show source code in whatlies/embeddingset.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __contains__ ( self , item ): \"\"\" Checks if an item is in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) \"foo\" in emb # True \"dinosaur\" in emb # False ``` \"\"\" return item in self . embeddings . keys () Checks if an item is in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) \"foo\" in emb # True \"dinosaur\" in emb # False __getitem__ ( self , thing ) \u00b6 Show source code in whatlies/embeddingset.py 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def __getitem__ ( self , thing ): \"\"\" Retreive a single embedding from the embeddingset. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz) emb[\"buz\"] ``` \"\"\" if isinstance ( thing , str ): return self . embeddings [ thing ] new_embeddings = { t : self [ t ] for t in thing } names = \",\" . join ( thing ) return EmbeddingSet ( new_embeddings , name = f \" { self . name } .subset( { names } )\" ) Retreive a single embedding from the embeddingset. Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) emb [ \"buz\" ] __iter__ ( self ) \u00b6 Show source code in whatlies/embeddingset.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def __iter__ ( self ): \"\"\" Iterate over all the embeddings in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) [e for e in emb] ``` \"\"\" return self . embeddings . values () . __iter__ () Iterate over all the embeddings in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) [ e for e in emb ] __or__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def __or__ ( self , other ): \"\"\" Makes every element in the embeddingset othogonal to the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb | buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb | other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } | { other . name } )\" ) Makes every element in the embeddingset othogonal to the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb | buz ) . plot ( kind = \"arrow\" ) __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def __rshift__ ( self , other ): \"\"\" Maps every embedding in the embedding set unto the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb >> buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb >> other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } >> { other . name } )\" ) Maps every embedding in the embedding set unto the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb >> buz ) . plot ( kind = \"arrow\" ) __sub__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def __sub__ ( self , other ): \"\"\" Subtracts an embedding from each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb - buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb - other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } - { other . name } )\" ) Subtracts an embedding from each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb - buz ) . plot ( kind = \"arrow\" ) add_property ( self , name , func ) \u00b6 Show source code in whatlies/embeddingset.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' ) assign ( self , ** kwargs ) \u00b6 Show source code in whatlies/embeddingset.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 def assign ( self , ** kwargs ): \"\"\" Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to `.add_property` but it might be more convenient when you want to assign multiple properties in one single statement. Arguments: kwargs: (name, func)-pairs that describe the name of the property as well as a function on how to calculate it. The function expects an `Embedding` object as input. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.assign(dim0=lambda d: d.vector[0], dim1=lambda d: d.vector[1], dim2=lambda d: d.vector[2]) ``` \"\"\" new_set = {} for k , e in self . embeddings . items (): new_emb = e for name , func in kwargs . items (): new_emb = new_emb . add_property ( name , func ) new_set [ k ] = new_emb return EmbeddingSet ( new_set ) Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to .add_property but it might be more convenient when you want to assign multiple properties in one single statement. Parameters Name Type Description Default **kwargs (name, func)-pairs that describe the name of the property as well as a function on how to calculate it. The function expects an Embedding object as input. {} Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . assign ( dim0 = lambda d : d . vector [ 0 ], dim1 = lambda d : d . vector [ 1 ], dim2 = lambda d : d . vector [ 2 ]) average ( self , name = None ) \u00b6 Show source code in whatlies/embeddingset.py 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = self . to_X () return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5] compare_against ( self , other , mapping = None ) \u00b6 Show source code in whatlies/embeddingset.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def compare_against ( self , other : Union [ str , Embedding ], mapping : Optional [ Callable ] = None ) -> List : \"\"\" Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Arguments: other: an `Embedding` instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. mapping: an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. `>` operator) is used. \"\"\" if isinstance ( other , str ): other = self [ other ] if mapping is None : return [ v > other for v in self . embeddings . values ()] elif callable ( mapping ): return [ mapping ( v . vector , other . vector ) for v in self . embeddings . values ()] else : raise ValueError ( f \"Unrecognized mapping value/type, got: { mapping } \" ) Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Parameters Name Type Description Default other Union[str, whatlies.embedding.Embedding] an Embedding instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. required mapping Optional[Callable] an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. > operator) is used. None embset_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 533 534 535 536 537 538 539 540 541 542 543 544 545 546 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings. filter ( self , func ) \u00b6 Show source code in whatlies/embeddingset.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def filter ( self , func ): \"\"\" Filters the collection of embeddings based on a predicate function. Arguments: func: callable that accepts a single embedding and outputs a boolean ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz, xyz) emb.filter(lambda e: \"foo\" not in e.name) ``` \"\"\" return EmbeddingSet ({ k : v for k , v in self . embeddings . items () if func ( v )}) Filters the collection of embeddings based on a predicate function. Parameters Name Type Description Default func callable that accepts a single embedding and outputs a boolean required from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz , xyz ) emb . filter ( lambda e : \"foo\" not in e . name ) from_names_X ( names , X ) (classmethod) \u00b6 Show source code in whatlies/embeddingset.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 @classmethod def from_names_X ( cls , names , X ): \"\"\" Constructs an `EmbeddingSet` instance from the given embedding names and vectors. Arguments: names: an iterable containing the names of embeddings X: an iterable of 1D vectors, or a 2D numpy array; it should have the same length as `names` Usage: ```python from whatlies.embeddingset import EmbeddingSet names = [\"foo\", \"bar\", \"buz\"] vecs = [ [0.1, 0.3], [0.7, 0.2], [0.1, 0.9], ] emb = EmbeddingSet.from_names_X(names, vecs) ``` \"\"\" X = np . array ( X ) if len ( X ) != len ( names ): raise ValueError ( f \"The number of given names ( { len ( names ) } ) and vectors ( { len ( X ) } ) should be the same.\" ) return cls ({ n : Embedding ( n , v ) for n , v in zip ( names , X )}) Constructs an EmbeddingSet instance from the given embedding names and vectors. Parameters Name Type Description Default names an iterable containing the names of embeddings required X an iterable of 1D vectors, or a 2D numpy array; it should have the same length as names required Usage: from whatlies.embeddingset import EmbeddingSet names = [ \"foo\" , \"bar\" , \"buz\" ] vecs = [ [ 0.1 , 0.3 ], [ 0.7 , 0.2 ], [ 0.1 , 0.9 ], ] emb = EmbeddingSet . from_names_X ( names , vecs ) merge ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = emb1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = emb1 . merge ( emb2 ) movement_df ( self , other , metric = 'euclidean' ) \u00b6 Show source code in whatlies/embeddingset.py 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 def movement_df ( self , other , metric = \"euclidean\" ): \"\"\" Creates a dataframe that shows the movement from one embeddingset to another one. Arguments: other: the other embeddingset to compare against, will only keep the overlap metric: metric to use to calculate movement, must be scipy or sklearn compatible Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb_ort = lang[names] | lang['cat'] emb.movement_df(emb_ort) ``` \"\"\" overlap = list ( set ( self . embeddings . keys ()) . intersection ( set ( other . embeddings . keys ())) ) mat1 = np . array ([ w . vector for w in self [ overlap ]]) mat2 = np . array ([ w . vector for w in other [ overlap ]]) return ( pd . DataFrame ( { \"name\" : overlap , \"movement\" : paired_distances ( mat1 , mat2 , metric )} ) . sort_values ([ \"movement\" ], ascending = False ) . reset_index () ) Creates a dataframe that shows the movement from one embeddingset to another one. Parameters Name Type Description Default other the other embeddingset to compare against, will only keep the overlap required metric metric to use to calculate movement, must be scipy or sklearn compatible 'euclidean' Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb_ort = lang [ names ] | lang [ 'cat' ] emb . movement_df ( emb_ort ) pipe ( self , func , * args , ** kwargs ) \u00b6 Show source code in whatlies/embeddingset.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def pipe ( self , func , * args , ** kwargs ): \"\"\" Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Arguments: func: callable that accepts an `EmbeddingSet` set as its first argument args: arguments to also pass to the function kwargs: keyword arguments to also pass to the function ```python from whatlies.language import SpacyLanguage, BytePairLanguage lang_sp = SpacyLanguage(\"en_core_web_sm\") lang_bp = BytePairLanguage(\"en\", dim=25, vs=1000) text = [\"cat\", \"dog\", \"rat\", \"blue\", \"red\", \"yellow\"] def make_plot(embset): return (embset .plot_interactive(\"dog\", \"blue\") .properties(height=200, width=200)) p1 = lang_sp[text].pipe(make_plot) p2 = lang_bp[text].pipe(make_plot) p1 | p2 ``` \"\"\" return func ( self , * args , ** kwargs ) Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Parameters Name Type Description Default func callable that accepts an EmbeddingSet set as its first argument required *args arguments to also pass to the function () **kwargs keyword arguments to also pass to the function {} from whatlies.language import SpacyLanguage , BytePairLanguage lang_sp = SpacyLanguage ( \"en_core_web_sm\" ) lang_bp = BytePairLanguage ( \"en\" , dim = 25 , vs = 1000 ) text = [ \"cat\" , \"dog\" , \"rat\" , \"blue\" , \"red\" , \"yellow\" ] def make_plot ( embset ): return ( embset . plot_interactive ( \"dog\" , \"blue\" ) . properties ( height = 200 , width = 200 )) p1 = lang_sp [ text ] . pipe ( make_plot ) p2 = lang_bp [ text ] . pipe ( make_plot ) p1 | p2 plot ( self , kind = 'arrow' , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , color = None , show_ops = False , annot = True , axis_option = None ) \u00b6 Show source code in whatlies/embeddingset.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric embeddings = [] for emb in self . embeddings . values (): x_val , x_lab = emb . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = emb . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) emb_plot = Embedding ( name = emb . name , vector = [ x_val , y_val ], orig = emb . orig ) embeddings . append ( emb_plot ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label handle_2d_plot ( embeddings , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None plot_3d ( self , x_axis = 0 , y_axis = 1 , z_axis = 2 , x_label = None , y_label = None , z_label = None , title = None , color = None , axis_metric = None , annot = True ) \u00b6 Show source code in whatlies/embeddingset.py 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 def plot_3d ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , z_axis : Union [ int , str , Embedding ] = 2 , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , z_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , ): \"\"\" Creates a 3d visualisation of the embedding. Arguments: x_axis: the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. z_axis: the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. z_label: an optional label used for z-axis; if not given, it is set based on value of `z_axis`. title: an optional title for the plot. color: the property to user for the color axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_3d(annot=True) emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\") emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\", axis_metric=\"cosine_distance\") ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( z_axis , str ): z_axis = self [ z_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] z_axis_metric = axis_metric [ 2 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric z_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name x_lab = x_label if x_label is not None else x_lab if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name y_lab = y_label if y_label is not None else y_lab if isinstance ( z_axis , int ): z_val = self . to_X ()[:, z_axis ] z_lab = \"Dimension \" + str ( z_axis ) else : z_axis_metric = Embedding . _get_plot_axis_metric_callable ( z_axis_metric ) z_val = self . compare_against ( z_axis , mapping = z_axis_metric ) z_lab = z_axis . name z_lab = z_label if z_label is not None else z_lab # Save relevant information in a dataframe for plotting later. plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"z_axis\" : z_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) # Deal with the colors of the dots. if color : plot_df [ \"color\" ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] color_map = { k : v for v , k in enumerate ( set ( plot_df [ \"color\" ]))} color_val = [ color_map [ k ] if not isinstance ( k , float ) else k for k in plot_df [ \"color\" ] ] else : color_val = None ax = plt . axes ( projection = \"3d\" ) ax . scatter3D ( plot_df [ \"x_axis\" ], plot_df [ \"y_axis\" ], plot_df [ \"z_axis\" ], c = color_val , s = 25 ) # Set the labels, titles, text annotations. ax . set_xlabel ( x_lab ) ax . set_ylabel ( y_lab ) ax . set_zlabel ( z_lab ) if annot : for i , row in plot_df . iterrows (): ax . text ( row [ \"x_axis\" ], row [ \"y_axis\" ], row [ \"z_axis\" ] + 0.05 , row [ \"original\" ] ) if title : ax . set_title ( label = title ) return ax Creates a 3d visualisation of the embedding. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 1 z_axis Union[int, str, whatlies.embedding.Embedding] the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 2 x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None z_label Optional[str] an optional label used for z-axis; if not given, it is set based on value of z_axis . None title Optional[str] an optional title for the plot. None color str the property to user for the color None axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_3d ( annot = True ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" , axis_metric = \"cosine_distance\" ) plot_correlation ( self , metric = None ) \u00b6 Show source code in whatlies/embeddingset.py 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 @deprecated ( \"This method will be deprecated in v0.6.0 in favor of `plot_distance` and `plot_similarity`\" ) def plot_correlation ( self , metric = None ): \"\"\" Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Arguments: metric: don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) Warning: This method will be deprecated in version 0.6.0 in favor of `plot_distance` and `plot_similarity`. Usage: ```python from whatlies.language import SpacyLanguage import matplotlib.pyplot as plt lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_correlation() ``` \"\"\" df = self . to_dataframe () . T corr_df = ( pairwise_distances ( self . to_matrix (), metric = metric ) if metric else df . corr () ) fig , ax = plt . subplots () plt . imshow ( corr_df ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) DEPRECATED: This method will be deprecated in v0.6.0 in favor of plot_distance and plot_similarity Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Parameters Name Type Description Default metric don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) None Warning This method will be deprecated in version 0.6.0 in favor of plot_distance and plot_similarity . Usage: from whatlies.language import SpacyLanguage import matplotlib.pyplot as plt lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_correlation () plot_distance ( self , metric = 'cosine' , norm = False ) \u00b6 Show source code in whatlies/embeddingset.py 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 def plot_distance ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a distance plot. Shows you the distance between all the word embeddings in the set. Arguments: metric: `'cosine'`, `'correlation'` or `'euclidean'` norm: normalise the vectors before calculating the distances Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_distance(metric='cosine') emb.plot_distance(metric='euclidean') emb.plot_distance(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" , \"euclidean\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : distances = cosine_distances ( X ) if metric == \"correlation\" : distances = 1 - np . corrcoef ( X ) vmin , vmax = - 1 , 1 if metric == \"euclidean\" : distances = euclidean_distances ( X ) vmin , vmax = 0 , np . max ( distances ) fig , ax = plt . subplots () plt . imshow ( distances , cmap = plt . cm . get_cmap () . reversed (), vmin = vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a distance plot. Shows you the distance between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' , 'correlation' or 'euclidean' 'cosine' norm normalise the vectors before calculating the distances False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_distance ( metric = 'cosine' ) emb . plot_distance ( metric = 'euclidean' ) emb . plot_distance ( metric = 'correlation' ) plot_interactive ( self , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , annot = True , color = None ) \u00b6 Show source code in whatlies/embeddingset.py 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 def plot_interactive ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , annot : bool = True , color : Union [ None , str ] = None , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot; if not given, it is set based on `x_axis` and `y_axis` values. annot: drawn points should be annotated color: a property that will be used for plotting **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name x_label = x_label if x_label is not None else x_lab y_label = y_label if y_label is not None else y_lab title = title if title is not None else f \" { x_lab } vs. { y_lab } \" plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_label )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_label )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = title ) . interactive () ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = \"x_axis\" , y = \"y_axis\" , text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot; if not given, it is set based on x_axis and y_axis values. None annot bool drawn points should be annotated True color Union[NoneType, str] a property that will be used for plotting None Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' ) plot_interactive_matrix ( self , * axes , axes_metric = None , annot = True , width = 200 , height = 200 ) \u00b6 Show source code in whatlies/embeddingset.py 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 def plot_interactive_matrix ( self , * axes : Union [ int , str , Embedding ], axes_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an `Embedding` instance (default: `0, 1`). axes_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as `axes` could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix(0, 1, 2) ``` \"\"\" # Set default value of axes, if not given. if len ( axes ) == 0 : axes = [ 0 , 1 ] if isinstance ( axes_metric , ( list , tuple )) and len ( axes_metric ) != len ( axes ): raise ValueError ( f \"The number of given axes metrics should be the same as the number of given axes. Got { len ( axes ) } axes vs. { len ( axes_metric ) } metrics.\" ) if not isinstance ( axes_metric , ( list , tuple )): axes_metric = [ axes_metric ] * len ( axes ) # Get values of each axis according to their type. axes_vals = {} X = self . to_X () for axis , metric in zip ( axes , axes_metric ): if isinstance ( axis , int ): vals = X [:, axis ] axes_vals [ \"Dimension \" + str ( axis )] = vals else : if isinstance ( axis , str ): axis = self [ axis ] metric = Embedding . _get_plot_axis_metric_callable ( metric ) vals = self . compare_against ( axis , mapping = metric ) axes_vals [ axis . name ] = vals plot_df = pd . DataFrame ( axes_vals ) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] axes_names = list ( axes_vals . keys ()) result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes_names [:: - 1 ], column = axes_names ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes Union[int, str, whatlies.embedding.Embedding] the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an Embedding instance (default: 0, 1 ). () axes_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as axes could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) plot_movement ( self , other , x_axis , y_axis , first_group_name = 'before' , second_group_name = 'after' , annot = True ) \u00b6 Show source code in whatlies/embeddingset.py 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_movement(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) ) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) . loc [ lambda d : d . index . isin ( df1 . index )] ) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ \"original\" ]) . assign ( constant = 1 ) ) plots = [] for idx , grp_df in df_draw . groupby ( \"original\" ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = \"x_axis:Q\" , y = \"y_axis:Q\" ) ) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_movement ( emb_new , 'man' , 'woman' ) plot_pixels ( self ) \u00b6 Show source code in whatlies/embeddingset.py 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 def plot_pixels ( self ): \"\"\" Makes a pixelchart of every embedding in the set. Usage: ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car', 'motor', 'cycle', 'firehydrant', 'japan', 'germany', 'belgium'] emb = lang[names].transform(Pca(12)).filter(lambda e: 'pca' not in e.name) emb.plot_pixels() ``` ![](https://rasahq.github.io/whatlies/images/pixels.png) \"\"\" names = self . embeddings . keys () df = self . to_dataframe () plt . matshow ( df ) plt . yticks ( range ( len ( names )), names ) Makes a pixelchart of every embedding in the set. Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' , 'motor' , 'cycle' , 'firehydrant' , 'japan' , 'germany' , 'belgium' ] emb = lang [ names ] . transform ( Pca ( 12 )) . filter ( lambda e : 'pca' not in e . name ) emb . plot_pixels () plot_similarity ( self , metric = 'cosine' , norm = False ) \u00b6 Show source code in whatlies/embeddingset.py 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 def plot_similarity ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Arguments: metric: `'cosine'` or `'correlation'` norm: normalise the embeddings before calculating the similarity Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_similarity() emb.plot_similarity(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : similarity = cosine_similarity ( X ) if metric == \"correlation\" : similarity = np . corrcoef ( X ) vmin , vmax = - 1 , 1 fig , ax = plt . subplots () plt . imshow ( similarity , cmap = plt . cm . get_cmap (), vmin =- vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' or 'correlation' 'cosine' norm normalise the embeddings before calculating the similarity False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_similarity () emb . plot_similarity ( metric = 'correlation' ) score_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if isinstance ( emb , str ): if emb not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { emb } ` does not exist in this EmbeddingSet\" ) emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = self . to_X () distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples. to_dataframe ( self ) \u00b6 Show source code in whatlies/embeddingset.py 585 586 587 588 589 590 def to_dataframe ( self ): \"\"\" Turns the embeddingset into a pandas dataframe. \"\"\" mat = self . to_matrix () return pd . DataFrame ( mat , index = list ( self . embeddings . keys ())) Turns the embeddingset into a pandas dataframe. to_matrix ( self ) \u00b6 Show source code in whatlies/embeddingset.py 579 580 581 582 583 def to_matrix ( self ): \"\"\" Does exactly the same as `.to_X`. It takes the embedding vectors and turns it into a numpy array. \"\"\" return self . to_X () Does exactly the same as .to_X . It takes the embedding vectors and turns it into a numpy array. to_names_X ( self ) \u00b6 Show source code in whatlies/embeddingset.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def to_names_X ( self ): \"\"\" Get the list of names as well as an array of vectors of all embeddings. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) names, X = emb.to_names_X() ``` \"\"\" return list ( self . embeddings . keys ()), self . to_X () Get the list of names as well as an array of vectors of all embeddings. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) names , X = emb . to_names_X () to_X ( self , norm = False ) \u00b6 Show source code in whatlies/embeddingset.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def to_X ( self , norm = False ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) X = normalize ( X ) if norm else X return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X () to_X_y ( self , y_label ) \u00b6 Show source code in whatlies/embeddingset.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = self . to_X () y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' ) transform ( self , transformer ) \u00b6 Show source code in whatlies/embeddingset.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"EmbeddingSet"},{"location":"api/embeddingset/#whatliesembeddingsetembeddingset","text":"This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of Embedding , or a single dictionary containing name: Embedding pairs name : custom name of embeddingset Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) emb = EmbeddingSet ( foo , bar ) emb = EmbeddingSet ({ 'foo' : foo , 'bar' : bar )","title":"whatlies.embeddingset.EmbeddingSet"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.ndim","text":"Return dimension of embedding vectors in embeddingset.","title":"ndim"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.add_property","text":"Show source code in whatlies/embeddingset.py 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' )","title":"add_property()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.assign","text":"Show source code in whatlies/embeddingset.py 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 def assign ( self , ** kwargs ): \"\"\" Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to `.add_property` but it might be more convenient when you want to assign multiple properties in one single statement. Arguments: kwargs: (name, func)-pairs that describe the name of the property as well as a function on how to calculate it. The function expects an `Embedding` object as input. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.assign(dim0=lambda d: d.vector[0], dim1=lambda d: d.vector[1], dim2=lambda d: d.vector[2]) ``` \"\"\" new_set = {} for k , e in self . embeddings . items (): new_emb = e for name , func in kwargs . items (): new_emb = new_emb . add_property ( name , func ) new_set [ k ] = new_emb return EmbeddingSet ( new_set ) Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to .add_property but it might be more convenient when you want to assign multiple properties in one single statement. Parameters Name Type Description Default **kwargs (name, func)-pairs that describe the name of the property as well as a function on how to calculate it. The function expects an Embedding object as input. {} Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . assign ( dim0 = lambda d : d . vector [ 0 ], dim1 = lambda d : d . vector [ 1 ], dim2 = lambda d : d . vector [ 2 ])","title":"assign()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.average","text":"Show source code in whatlies/embeddingset.py 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = self . to_X () return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5]","title":"average()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.compare_against","text":"Show source code in whatlies/embeddingset.py 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 def compare_against ( self , other : Union [ str , Embedding ], mapping : Optional [ Callable ] = None ) -> List : \"\"\" Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Arguments: other: an `Embedding` instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. mapping: an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. `>` operator) is used. \"\"\" if isinstance ( other , str ): other = self [ other ] if mapping is None : return [ v > other for v in self . embeddings . values ()] elif callable ( mapping ): return [ mapping ( v . vector , other . vector ) for v in self . embeddings . values ()] else : raise ValueError ( f \"Unrecognized mapping value/type, got: { mapping } \" ) Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Parameters Name Type Description Default other Union[str, whatlies.embedding.Embedding] an Embedding instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. required mapping Optional[Callable] an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. > operator) is used. None","title":"compare_against()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.embset_similar","text":"Show source code in whatlies/embeddingset.py 533 534 535 536 537 538 539 540 541 542 543 544 545 546 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.filter","text":"Show source code in whatlies/embeddingset.py 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 def filter ( self , func ): \"\"\" Filters the collection of embeddings based on a predicate function. Arguments: func: callable that accepts a single embedding and outputs a boolean ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz, xyz) emb.filter(lambda e: \"foo\" not in e.name) ``` \"\"\" return EmbeddingSet ({ k : v for k , v in self . embeddings . items () if func ( v )}) Filters the collection of embeddings based on a predicate function. Parameters Name Type Description Default func callable that accepts a single embedding and outputs a boolean required from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz , xyz ) emb . filter ( lambda e : \"foo\" not in e . name )","title":"filter()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.from_names_X","text":"Show source code in whatlies/embeddingset.py 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 @classmethod def from_names_X ( cls , names , X ): \"\"\" Constructs an `EmbeddingSet` instance from the given embedding names and vectors. Arguments: names: an iterable containing the names of embeddings X: an iterable of 1D vectors, or a 2D numpy array; it should have the same length as `names` Usage: ```python from whatlies.embeddingset import EmbeddingSet names = [\"foo\", \"bar\", \"buz\"] vecs = [ [0.1, 0.3], [0.7, 0.2], [0.1, 0.9], ] emb = EmbeddingSet.from_names_X(names, vecs) ``` \"\"\" X = np . array ( X ) if len ( X ) != len ( names ): raise ValueError ( f \"The number of given names ( { len ( names ) } ) and vectors ( { len ( X ) } ) should be the same.\" ) return cls ({ n : Embedding ( n , v ) for n , v in zip ( names , X )}) Constructs an EmbeddingSet instance from the given embedding names and vectors. Parameters Name Type Description Default names an iterable containing the names of embeddings required X an iterable of 1D vectors, or a 2D numpy array; it should have the same length as names required Usage: from whatlies.embeddingset import EmbeddingSet names = [ \"foo\" , \"bar\" , \"buz\" ] vecs = [ [ 0.1 , 0.3 ], [ 0.7 , 0.2 ], [ 0.1 , 0.9 ], ] emb = EmbeddingSet . from_names_X ( names , vecs )","title":"from_names_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.merge","text":"Show source code in whatlies/embeddingset.py 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = emb1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = emb1 . merge ( emb2 )","title":"merge()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.movement_df","text":"Show source code in whatlies/embeddingset.py 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 def movement_df ( self , other , metric = \"euclidean\" ): \"\"\" Creates a dataframe that shows the movement from one embeddingset to another one. Arguments: other: the other embeddingset to compare against, will only keep the overlap metric: metric to use to calculate movement, must be scipy or sklearn compatible Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb_ort = lang[names] | lang['cat'] emb.movement_df(emb_ort) ``` \"\"\" overlap = list ( set ( self . embeddings . keys ()) . intersection ( set ( other . embeddings . keys ())) ) mat1 = np . array ([ w . vector for w in self [ overlap ]]) mat2 = np . array ([ w . vector for w in other [ overlap ]]) return ( pd . DataFrame ( { \"name\" : overlap , \"movement\" : paired_distances ( mat1 , mat2 , metric )} ) . sort_values ([ \"movement\" ], ascending = False ) . reset_index () ) Creates a dataframe that shows the movement from one embeddingset to another one. Parameters Name Type Description Default other the other embeddingset to compare against, will only keep the overlap required metric metric to use to calculate movement, must be scipy or sklearn compatible 'euclidean' Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb_ort = lang [ names ] | lang [ 'cat' ] emb . movement_df ( emb_ort )","title":"movement_df()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.pipe","text":"Show source code in whatlies/embeddingset.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 def pipe ( self , func , * args , ** kwargs ): \"\"\" Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Arguments: func: callable that accepts an `EmbeddingSet` set as its first argument args: arguments to also pass to the function kwargs: keyword arguments to also pass to the function ```python from whatlies.language import SpacyLanguage, BytePairLanguage lang_sp = SpacyLanguage(\"en_core_web_sm\") lang_bp = BytePairLanguage(\"en\", dim=25, vs=1000) text = [\"cat\", \"dog\", \"rat\", \"blue\", \"red\", \"yellow\"] def make_plot(embset): return (embset .plot_interactive(\"dog\", \"blue\") .properties(height=200, width=200)) p1 = lang_sp[text].pipe(make_plot) p2 = lang_bp[text].pipe(make_plot) p1 | p2 ``` \"\"\" return func ( self , * args , ** kwargs ) Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Parameters Name Type Description Default func callable that accepts an EmbeddingSet set as its first argument required *args arguments to also pass to the function () **kwargs keyword arguments to also pass to the function {} from whatlies.language import SpacyLanguage , BytePairLanguage lang_sp = SpacyLanguage ( \"en_core_web_sm\" ) lang_bp = BytePairLanguage ( \"en\" , dim = 25 , vs = 1000 ) text = [ \"cat\" , \"dog\" , \"rat\" , \"blue\" , \"red\" , \"yellow\" ] def make_plot ( embset ): return ( embset . plot_interactive ( \"dog\" , \"blue\" ) . properties ( height = 200 , width = 200 )) p1 = lang_sp [ text ] . pipe ( make_plot ) p2 = lang_bp [ text ] . pipe ( make_plot ) p1 | p2","title":"pipe()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot","text":"Show source code in whatlies/embeddingset.py 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric embeddings = [] for emb in self . embeddings . values (): x_val , x_lab = emb . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = emb . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) emb_plot = Embedding ( name = emb . name , vector = [ x_val , y_val ], orig = emb . orig ) embeddings . append ( emb_plot ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label handle_2d_plot ( embeddings , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None","title":"plot()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_3d","text":"Show source code in whatlies/embeddingset.py 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 def plot_3d ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , z_axis : Union [ int , str , Embedding ] = 2 , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , z_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , ): \"\"\" Creates a 3d visualisation of the embedding. Arguments: x_axis: the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. z_axis: the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. z_label: an optional label used for z-axis; if not given, it is set based on value of `z_axis`. title: an optional title for the plot. color: the property to user for the color axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_3d(annot=True) emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\") emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\", axis_metric=\"cosine_distance\") ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( z_axis , str ): z_axis = self [ z_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] z_axis_metric = axis_metric [ 2 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric z_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name x_lab = x_label if x_label is not None else x_lab if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name y_lab = y_label if y_label is not None else y_lab if isinstance ( z_axis , int ): z_val = self . to_X ()[:, z_axis ] z_lab = \"Dimension \" + str ( z_axis ) else : z_axis_metric = Embedding . _get_plot_axis_metric_callable ( z_axis_metric ) z_val = self . compare_against ( z_axis , mapping = z_axis_metric ) z_lab = z_axis . name z_lab = z_label if z_label is not None else z_lab # Save relevant information in a dataframe for plotting later. plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"z_axis\" : z_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) # Deal with the colors of the dots. if color : plot_df [ \"color\" ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] color_map = { k : v for v , k in enumerate ( set ( plot_df [ \"color\" ]))} color_val = [ color_map [ k ] if not isinstance ( k , float ) else k for k in plot_df [ \"color\" ] ] else : color_val = None ax = plt . axes ( projection = \"3d\" ) ax . scatter3D ( plot_df [ \"x_axis\" ], plot_df [ \"y_axis\" ], plot_df [ \"z_axis\" ], c = color_val , s = 25 ) # Set the labels, titles, text annotations. ax . set_xlabel ( x_lab ) ax . set_ylabel ( y_lab ) ax . set_zlabel ( z_lab ) if annot : for i , row in plot_df . iterrows (): ax . text ( row [ \"x_axis\" ], row [ \"y_axis\" ], row [ \"z_axis\" ] + 0.05 , row [ \"original\" ] ) if title : ax . set_title ( label = title ) return ax Creates a 3d visualisation of the embedding. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 1 z_axis Union[int, str, whatlies.embedding.Embedding] the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 2 x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None z_label Optional[str] an optional label used for z-axis; if not given, it is set based on value of z_axis . None title Optional[str] an optional title for the plot. None color str the property to user for the color None axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_3d ( annot = True ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" , axis_metric = \"cosine_distance\" )","title":"plot_3d()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_correlation","text":"Show source code in whatlies/embeddingset.py 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 @deprecated ( \"This method will be deprecated in v0.6.0 in favor of `plot_distance` and `plot_similarity`\" ) def plot_correlation ( self , metric = None ): \"\"\" Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Arguments: metric: don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) Warning: This method will be deprecated in version 0.6.0 in favor of `plot_distance` and `plot_similarity`. Usage: ```python from whatlies.language import SpacyLanguage import matplotlib.pyplot as plt lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_correlation() ``` \"\"\" df = self . to_dataframe () . T corr_df = ( pairwise_distances ( self . to_matrix (), metric = metric ) if metric else df . corr () ) fig , ax = plt . subplots () plt . imshow ( corr_df ) plt . xticks ( range ( len ( df . columns )), df . columns ) plt . yticks ( range ( len ( df . columns )), df . columns ) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) DEPRECATED: This method will be deprecated in v0.6.0 in favor of plot_distance and plot_similarity Make a correlation plot. Shows you the correlation between all the word embeddings. Can also be configured to show distances instead. Parameters Name Type Description Default metric don't plot correlation but a distance measure, must be scipy compatible (cosine, euclidean, etc) None Warning This method will be deprecated in version 0.6.0 in favor of plot_distance and plot_similarity . Usage: from whatlies.language import SpacyLanguage import matplotlib.pyplot as plt lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_correlation ()","title":"plot_correlation()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_distance","text":"Show source code in whatlies/embeddingset.py 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 def plot_distance ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a distance plot. Shows you the distance between all the word embeddings in the set. Arguments: metric: `'cosine'`, `'correlation'` or `'euclidean'` norm: normalise the vectors before calculating the distances Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_distance(metric='cosine') emb.plot_distance(metric='euclidean') emb.plot_distance(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" , \"euclidean\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : distances = cosine_distances ( X ) if metric == \"correlation\" : distances = 1 - np . corrcoef ( X ) vmin , vmax = - 1 , 1 if metric == \"euclidean\" : distances = euclidean_distances ( X ) vmin , vmax = 0 , np . max ( distances ) fig , ax = plt . subplots () plt . imshow ( distances , cmap = plt . cm . get_cmap () . reversed (), vmin = vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a distance plot. Shows you the distance between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' , 'correlation' or 'euclidean' 'cosine' norm normalise the vectors before calculating the distances False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_distance ( metric = 'cosine' ) emb . plot_distance ( metric = 'euclidean' ) emb . plot_distance ( metric = 'correlation' )","title":"plot_distance()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive","text":"Show source code in whatlies/embeddingset.py 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 def plot_interactive ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , annot : bool = True , color : Union [ None , str ] = None , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot; if not given, it is set based on `x_axis` and `y_axis` values. annot: drawn points should be annotated color: a property that will be used for plotting **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name x_label = x_label if x_label is not None else x_lab y_label = y_label if y_label is not None else y_lab title = title if title is not None else f \" { x_lab } vs. { y_lab } \" plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_label )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_label )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = title ) . interactive () ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = \"x_axis\" , y = \"y_axis\" , text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot; if not given, it is set based on x_axis and y_axis values. None annot bool drawn points should be annotated True color Union[NoneType, str] a property that will be used for plotting None Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' )","title":"plot_interactive()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive_matrix","text":"Show source code in whatlies/embeddingset.py 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 def plot_interactive_matrix ( self , * axes : Union [ int , str , Embedding ], axes_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an `Embedding` instance (default: `0, 1`). axes_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as `axes` could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix(0, 1, 2) ``` \"\"\" # Set default value of axes, if not given. if len ( axes ) == 0 : axes = [ 0 , 1 ] if isinstance ( axes_metric , ( list , tuple )) and len ( axes_metric ) != len ( axes ): raise ValueError ( f \"The number of given axes metrics should be the same as the number of given axes. Got { len ( axes ) } axes vs. { len ( axes_metric ) } metrics.\" ) if not isinstance ( axes_metric , ( list , tuple )): axes_metric = [ axes_metric ] * len ( axes ) # Get values of each axis according to their type. axes_vals = {} X = self . to_X () for axis , metric in zip ( axes , axes_metric ): if isinstance ( axis , int ): vals = X [:, axis ] axes_vals [ \"Dimension \" + str ( axis )] = vals else : if isinstance ( axis , str ): axis = self [ axis ] metric = Embedding . _get_plot_axis_metric_callable ( metric ) vals = self . compare_against ( axis , mapping = metric ) axes_vals [ axis . name ] = vals plot_df = pd . DataFrame ( axes_vals ) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] axes_names = list ( axes_vals . keys ()) result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes_names [:: - 1 ], column = axes_names ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes Union[int, str, whatlies.embedding.Embedding] the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an Embedding instance (default: 0, 1 ). () axes_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as axes could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"plot_interactive_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_movement","text":"Show source code in whatlies/embeddingset.py 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"bluee\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_movement(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) ) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) . loc [ lambda d : d . index . isin ( df1 . index )] ) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ \"original\" ]) . assign ( constant = 1 ) ) plots = [] for idx , grp_df in df_draw . groupby ( \"original\" ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = \"x_axis:Q\" , y = \"y_axis:Q\" ) ) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_movement ( emb_new , 'man' , 'woman' )","title":"plot_movement()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_pixels","text":"Show source code in whatlies/embeddingset.py 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 def plot_pixels ( self ): \"\"\" Makes a pixelchart of every embedding in the set. Usage: ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car', 'motor', 'cycle', 'firehydrant', 'japan', 'germany', 'belgium'] emb = lang[names].transform(Pca(12)).filter(lambda e: 'pca' not in e.name) emb.plot_pixels() ``` ![](https://rasahq.github.io/whatlies/images/pixels.png) \"\"\" names = self . embeddings . keys () df = self . to_dataframe () plt . matshow ( df ) plt . yticks ( range ( len ( names )), names ) Makes a pixelchart of every embedding in the set. Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' , 'motor' , 'cycle' , 'firehydrant' , 'japan' , 'germany' , 'belgium' ] emb = lang [ names ] . transform ( Pca ( 12 )) . filter ( lambda e : 'pca' not in e . name ) emb . plot_pixels ()","title":"plot_pixels()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_similarity","text":"Show source code in whatlies/embeddingset.py 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 def plot_similarity ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Arguments: metric: `'cosine'` or `'correlation'` norm: normalise the embeddings before calculating the similarity Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_similarity() emb.plot_similarity(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : similarity = cosine_similarity ( X ) if metric == \"correlation\" : similarity = np . corrcoef ( X ) vmin , vmax = - 1 , 1 fig , ax = plt . subplots () plt . imshow ( similarity , cmap = plt . cm . get_cmap (), vmin =- vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' or 'correlation' 'cosine' norm normalise the embeddings before calculating the similarity False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_similarity () emb . plot_similarity ( metric = 'correlation' )","title":"plot_similarity()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.score_similar","text":"Show source code in whatlies/embeddingset.py 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if isinstance ( emb , str ): if emb not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { emb } ` does not exist in this EmbeddingSet\" ) emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = self . to_X () distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_dataframe","text":"Show source code in whatlies/embeddingset.py 585 586 587 588 589 590 def to_dataframe ( self ): \"\"\" Turns the embeddingset into a pandas dataframe. \"\"\" mat = self . to_matrix () return pd . DataFrame ( mat , index = list ( self . embeddings . keys ())) Turns the embeddingset into a pandas dataframe.","title":"to_dataframe()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_matrix","text":"Show source code in whatlies/embeddingset.py 579 580 581 582 583 def to_matrix ( self ): \"\"\" Does exactly the same as `.to_X`. It takes the embedding vectors and turns it into a numpy array. \"\"\" return self . to_X () Does exactly the same as .to_X . It takes the embedding vectors and turns it into a numpy array.","title":"to_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_names_X","text":"Show source code in whatlies/embeddingset.py 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 def to_names_X ( self ): \"\"\" Get the list of names as well as an array of vectors of all embeddings. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) names, X = emb.to_names_X() ``` \"\"\" return list ( self . embeddings . keys ()), self . to_X () Get the list of names as well as an array of vectors of all embeddings. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) names , X = emb . to_names_X ()","title":"to_names_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X","text":"Show source code in whatlies/embeddingset.py 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 def to_X ( self , norm = False ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) X = normalize ( X ) if norm else X return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X ()","title":"to_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X_y","text":"Show source code in whatlies/embeddingset.py 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = self . to_X () y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' )","title":"to_X_y()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.transform","text":"Show source code in whatlies/embeddingset.py 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"transform()"},{"location":"api/language/bpemb_lang/","text":"whatlies.language.BytePairLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a Byte-Pair Encoding backend. This object is meant for retreival, not plotting. This language represents token-free pre-trained subword embeddings. Originally created by Benjamin Heinzerling and Michael Strube. Important These vectors will auto-download by the BPEmb package . You can also specify \"multi\" to download multi language embeddings. A full list of available languages can be found here . The article that belongs to this work can be found here Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. The availability of vocabulary size as well as dimensionality can be varified on the project website. See here for an example link in English. Please credit the original authors if you use their work. Warning This class used to be called BytePairLang . Parameters Name Type Description Default lang name of the model to load required vs vocabulary size of the byte pair model 10000 dim the embedding dimensionality 100 cache_dir The folder in which downloaded BPEmb files will be cached PosixPath('/Users/vincent/.cache/bpemb') Typically the vocabulary size given from this backend can be of size 1000, 3000, 5000, 10000, 25000, 50000, 100000 or 200000. The available dimensionality of the embbeddings typically are 25, 50, 100, 200 and 300. Usage : > from whatlies.language import BytePairLanguage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang = BytePairLanguage ( lang = \"multi\" ) > lang [[ 'hund' , 'hond' , 'dog' ]] __getitem__ ( self , item ) \u00b6 Show source code in language/_bpemblang.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def __getitem__ ( self , item ): \"\"\" Retreive a single embedding or a set of embeddings. If an embedding contains multiple sub-tokens then we'll average them before retreival. Arguments: item: single string or list of strings **Usage** ```python > lang = BytePairLanguage(lang=\"en\") > lang['python'] > lang[['python', 'snake']] > lang[['nobody expects', 'the spanish inquisition']] ``` \"\"\" if isinstance ( item , str ): with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = RuntimeWarning ) return Embedding ( item , self . module . embed ( item ) . mean ( axis = 0 )) if isinstance ( item , list ): return EmbeddingSet ( * [ self [ i ] for i in item ]) raise ValueError ( f \"Item must be list of string got { item } .\" ) Retreive a single embedding or a set of embeddings. If an embedding contains multiple sub-tokens then we'll average them before retreival. Parameters Name Type Description Default item single string or list of strings required Usage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' ]] > lang [[ 'nobody expects' , 'the spanish inquisition' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_bpemblang.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_bpemblang.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"BytePair"},{"location":"api/language/bpemb_lang/#whatlieslanguagebytepairlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a Byte-Pair Encoding backend. This object is meant for retreival, not plotting. This language represents token-free pre-trained subword embeddings. Originally created by Benjamin Heinzerling and Michael Strube. Important These vectors will auto-download by the BPEmb package . You can also specify \"multi\" to download multi language embeddings. A full list of available languages can be found here . The article that belongs to this work can be found here Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. The availability of vocabulary size as well as dimensionality can be varified on the project website. See here for an example link in English. Please credit the original authors if you use their work. Warning This class used to be called BytePairLang . Parameters Name Type Description Default lang name of the model to load required vs vocabulary size of the byte pair model 10000 dim the embedding dimensionality 100 cache_dir The folder in which downloaded BPEmb files will be cached PosixPath('/Users/vincent/.cache/bpemb') Typically the vocabulary size given from this backend can be of size 1000, 3000, 5000, 10000, 25000, 50000, 100000 or 200000. The available dimensionality of the embbeddings typically are 25, 50, 100, 200 and 300. Usage : > from whatlies.language import BytePairLanguage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang = BytePairLanguage ( lang = \"multi\" ) > lang [[ 'hund' , 'hond' , 'dog' ]]","title":"whatlies.language.BytePairLanguage"},{"location":"api/language/bpemb_lang/#whatlies.language._bpemblang.BytePairLanguage.embset_similar","text":"Show source code in language/_bpemblang.py 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/bpemb_lang/#whatlies.language._bpemblang.BytePairLanguage.score_similar","text":"Show source code in language/_bpemblang.py 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/convert_lang/","text":"whatlies.language.ConveRTLanguage \u00b6 This object is used to fetch Embedding s or EmbeddingSet s from a ConveRT model. This object is meant for retreival, not plotting. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Parameters Name Type Description Default model_id str identifier used for loading the corresponding TFHub module, which could be one of 'convert , 'convert-multi-context' or 'convert-ubuntu' . Each one of these correspond to a different model as described in ConveRT manual . 'convert' signature str the TFHub signature of the model, which could be one of 'default' , 'encode_context' , 'encode_response' or 'encode_sequence' . Note that 'encode_context' is not currently supported with 'convert-multi-context' or 'convert-ubuntu' models. 'default' Usage : > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage ( model_id = 'convert-multi-context' , signature = 'encode_sequence' ) > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_convert_lang.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage() > lang['bank'] > lang = ConveRTLanguage() > lang[['bank of the river', 'money on the bank', 'bank']] ``` \"\"\" if isinstance ( query , str ): query_tensor = tf . convert_to_tensor ([ query ]) encoding = self . model ( query_tensor ) if self . signature == \"encode_sequence\" : vec = encoding [ \"sequence_encoding\" ] . numpy () . sum ( axis = 1 )[ 0 ] else : vec = encoding [ \"default\" ] . numpy ()[ 0 ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage () > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]]","title":"ConveRT"},{"location":"api/language/convert_lang/#whatlieslanguageconvertlanguage","text":"This object is used to fetch Embedding s or EmbeddingSet s from a ConveRT model. This object is meant for retreival, not plotting. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Parameters Name Type Description Default model_id str identifier used for loading the corresponding TFHub module, which could be one of 'convert , 'convert-multi-context' or 'convert-ubuntu' . Each one of these correspond to a different model as described in ConveRT manual . 'convert' signature str the TFHub signature of the model, which could be one of 'default' , 'encode_context' , 'encode_response' or 'encode_sequence' . Note that 'encode_context' is not currently supported with 'convert-multi-context' or 'convert-ubuntu' models. 'default' Usage : > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage ( model_id = 'convert-multi-context' , signature = 'encode_sequence' ) > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]]","title":"whatlies.language.ConveRTLanguage"},{"location":"api/language/countvector_lang/","text":"whatlies.language.CountVectorLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a countvector language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn CountVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_countvector_lang.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a set of embeddings. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang[['pizza', 'pizzas', 'firehouse', 'firehydrant']] ``` \"\"\" orig_str = isinstance ( query , str ) if orig_str : query = [ query ] if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) if self . fitted_manual : X = self . cv . transform ( query ) X_vec = self . svd . transform ( X ) else : X = self . cv . fit_transform ( query ) X_vec = self . svd . fit_transform ( X ) if orig_str : return Embedding ( name = query [ 0 ], vector = X_vec [ 0 ]) return EmbeddingSet ( * [ Embedding ( name = n , vector = v ) for n , v in zip ( query , X_vec )] ) Retreive a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_countvector_lang.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. fit_manual ( self , query ) \u00b6 Show source code in language/_countvector_lang.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) > lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) > lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]] score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_countvector_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"CountVector"},{"location":"api/language/countvector_lang/#whatlieslanguagecountvectorlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a countvector language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn CountVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]]","title":"whatlies.language.CountVectorLanguage"},{"location":"api/language/countvector_lang/#whatlies.language._countvector_lang.CountVectorLanguage.embset_similar","text":"Show source code in language/_countvector_lang.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/countvector_lang/#whatlies.language._countvector_lang.CountVectorLanguage.fit_manual","text":"Show source code in language/_countvector_lang.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) > lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) > lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]]","title":"fit_manual()"},{"location":"api/language/countvector_lang/#whatlies.language._countvector_lang.CountVectorLanguage.score_similar","text":"Show source code in language/_countvector_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/fasttext_lang/","text":"whatlies.language.FasttextLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download upfront. You can find the download links here . Note: you'll want the bin file, not the text file. To train your own fasttext model see the guide here . Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_fasttext_lang.py 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = FasttextLanguage(\"cc.en.300.bin\") > lang['python'] > lang[['python'], ['snake']] > lang[['nobody expects'], ['the spanish inquisition']] ``` \"\"\" if isinstance ( query , str ): self . _input_str_legal ( query ) vec = self . model . get_word_vector ( query ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang [[ 'python' ], [ 'snake' ]] > lang [[ 'nobody expects' ], [ 'the spanish inquisition' ]] embset_proximity ( self , emb , max_proximity = 0.1 , top_n = 20000 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_fasttext_lang.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_fasttext_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_fasttext_lang.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"fasttext"},{"location":"api/language/fasttext_lang/#whatlieslanguagefasttextlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download upfront. You can find the download links here . Note: you'll want the bin file, not the text file. To train your own fasttext model see the guide here . Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]]","title":"whatlies.language.FasttextLanguage"},{"location":"api/language/fasttext_lang/#whatlies.language._fasttext_lang.FasttextLanguage.embset_proximity","text":"Show source code in language/_fasttext_lang.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/fasttext_lang/#whatlies.language._fasttext_lang.FasttextLanguage.embset_similar","text":"Show source code in language/_fasttext_lang.py 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/fasttext_lang/#whatlies.language._fasttext_lang.FasttextLanguage.score_similar","text":"Show source code in language/_fasttext_lang.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/gensim_lang/","text":"whatlies.language.GensimLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a keyed vector file. These files are generated by gensim . This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download/created upfront. A potential benefit of this is that you can train your own embeddings using gensim and visualise them using this library. Here's a snippet that you can use to train your own (very limited) word2vec embeddings. from gensim.test.utils import common_texts from gensim.models import Word2Vec model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4) model.wv.save(\"wordvectors.kv\") You can also download pre-trained embeddings that are hosted by the gensim project. import gensim.downloader as api # To check what models are available api.info()['models'].keys() # To download the vectors wv = api.load('glove-twitter-25') # This is typically saved in `~/gensim/data` but you can also edit these # vectors and save them someplace else if you'd like. wv.save(\"glove-twitter-25.kv\") Note that if a word is not available in the keyed vectors file then we'll assume a zero vector. If you pass a sentence then we'll add together the embeddings vectors of the seperate words. Parameters Name Type Description Default keyedfile name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_gensim_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import GensimLanguage > lang = GensimLanguage(\"wordvectors.kv\") > lang['computer'] > lang = GensimLanguage(\"wordvectors.kv\") > lang[['computer', 'human', 'dog']] ``` \"\"\" if isinstance ( query , str ): if \" \" in query : return Embedding ( query , np . sum ([ self [ q ] . vector for q in query . split ( \" \" )], axis = 0 ) ) try : vec = np . sum ([ self . kv [ q ] for q in query . split ( \" \" )], axis = 0 ) except KeyError : vec = np . zeros ( self . kv . vector_size ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_gensim_lang.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_gensim_lang.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"Gensim"},{"location":"api/language/gensim_lang/#whatlieslanguagegensimlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a keyed vector file. These files are generated by gensim . This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download/created upfront. A potential benefit of this is that you can train your own embeddings using gensim and visualise them using this library. Here's a snippet that you can use to train your own (very limited) word2vec embeddings. from gensim.test.utils import common_texts from gensim.models import Word2Vec model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4) model.wv.save(\"wordvectors.kv\") You can also download pre-trained embeddings that are hosted by the gensim project. import gensim.downloader as api # To check what models are available api.info()['models'].keys() # To download the vectors wv = api.load('glove-twitter-25') # This is typically saved in `~/gensim/data` but you can also edit these # vectors and save them someplace else if you'd like. wv.save(\"glove-twitter-25.kv\") Note that if a word is not available in the keyed vectors file then we'll assume a zero vector. If you pass a sentence then we'll add together the embeddings vectors of the seperate words. Parameters Name Type Description Default keyedfile name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]]","title":"whatlies.language.GensimLanguage"},{"location":"api/language/gensim_lang/#whatlies.language._gensim_lang.GensimLanguage.embset_similar","text":"Show source code in language/_gensim_lang.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/gensim_lang/#whatlies.language._gensim_lang.GensimLanguage.score_similar","text":"Show source code in language/_gensim_lang.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/s2v_lang/","text":"whatlies.language.Sense2VecLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here . __getitem__ ( self , query ) \u00b6 Show source code in language/_sense2vec_lang.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def __getitem__ ( self , query ): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['duck|NOUN'] > lang[['duck|NOUN'], ['duck|VERB']] ``` \"\"\" if isinstance ( query , str ): vec = self . s2v [ query ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'duck|NOUN' ] > lang [[ 'duck|NOUN' ], [ 'duck|VERB' ]] embset_similar ( self , query , n = 10 ) \u00b6 Show source code in language/_sense2vec_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , query , n = 10 ) \u00b6 Show source code in language/_sense2vec_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"Sense2Vec"},{"location":"api/language/s2v_lang/#whatlieslanguagesense2veclanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here .","title":"whatlies.language.Sense2VecLanguage"},{"location":"api/language/s2v_lang/#whatlies.language._sense2vec_lang.Sense2VecLanguage.embset_similar","text":"Show source code in language/_sense2vec_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/s2v_lang/#whatlies.language._sense2vec_lang.Sense2VecLanguage.score_similar","text":"Show source code in language/_sense2vec_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/spacy_lang/","text":"whatlies.language.SpacyLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default nlp Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ] __getitem__ ( self , query ) \u00b6 Show source code in language/_spacy_lang.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['python'] > lang[['python', 'snake']] > lang[['nobody expects', 'the spanish inquisition']] > lang = SpacyLanguage(\"en_trf_robertabase_lg\") > lang['programming in [python]'] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' ]] > lang [[ 'nobody expects' , 'the spanish inquisition' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ] embset_proximity ( self , emb , max_proximity = 0.1 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_spacy_lang.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_spacy_lang.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. from_fasttext ( language , output_dir , vectors_loc = None , force = False ) (classmethod) \u00b6 Show source code in language/_spacy_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @classmethod def from_fasttext ( cls , language , output_dir , vectors_loc = None , force = False ): \"\"\" Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important: The fasttext vectors are not given by this library. You can download the models [here](https://fasttext.cc/docs/en/crawl-vectors.html#models). Note that these files are large and loading them can take a long time. Arguments: language: name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) output_dir: directory to save spaCy model vectors_loc: file containing the fasttext vectors force: with this flag raised we will always recreate the model from the vec.gz file **Usage**: ```python > lang = SpacyLanguage.from_fasttext(\"nl\", \"/path/spacy/model\", \"~/Downloads/cc.nl.300.vec.gz\") > lang = SpacyLanguage.from_fasttext(\"en\", \"/path/spacy/model\", \"~/Downloads/cc.en.300.vec.gz\") ``` \"\"\" if not os . path . exists ( output_dir ): spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) else : if force : spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) return SpacyLanguage ( spacy . load ( output_dir )) Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important The fasttext vectors are not given by this library. You can download the models here . Note that these files are large and loading them can take a long time. Parameters Name Type Description Default language name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) required output_dir directory to save spaCy model required vectors_loc file containing the fasttext vectors None force with this flag raised we will always recreate the model from the vec.gz file False Usage : > lang = SpacyLanguage . from_fasttext ( \"nl\" , \"/path/spacy/model\" , \"~/Downloads/cc.nl.300.vec.gz\" ) > lang = SpacyLanguage . from_fasttext ( \"en\" , \"/path/spacy/model\" , \"~/Downloads/cc.en.300.vec.gz\" ) score_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_spacy_lang.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning , ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"spaCy"},{"location":"api/language/spacy_lang/#whatlieslanguagespacylanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default nlp Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ]","title":"whatlies.language.SpacyLanguage"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.embset_proximity","text":"Show source code in language/_spacy_lang.py 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.embset_similar","text":"Show source code in language/_spacy_lang.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.from_fasttext","text":"Show source code in language/_spacy_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 @classmethod def from_fasttext ( cls , language , output_dir , vectors_loc = None , force = False ): \"\"\" Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important: The fasttext vectors are not given by this library. You can download the models [here](https://fasttext.cc/docs/en/crawl-vectors.html#models). Note that these files are large and loading them can take a long time. Arguments: language: name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) output_dir: directory to save spaCy model vectors_loc: file containing the fasttext vectors force: with this flag raised we will always recreate the model from the vec.gz file **Usage**: ```python > lang = SpacyLanguage.from_fasttext(\"nl\", \"/path/spacy/model\", \"~/Downloads/cc.nl.300.vec.gz\") > lang = SpacyLanguage.from_fasttext(\"en\", \"/path/spacy/model\", \"~/Downloads/cc.en.300.vec.gz\") ``` \"\"\" if not os . path . exists ( output_dir ): spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) else : if force : spacy . cli . init_model ( lang = language , output_dir = output_dir , vectors_loc = vectors_loc ) return SpacyLanguage ( spacy . load ( output_dir )) Will load downloaded fasttext vectors. It will try to load from disk, but if there is no local spaCy model then we will first convert from the vec.gz file into a spaCy model. This is saved on disk and then loaded as a spaCy model. Important The fasttext vectors are not given by this library. You can download the models here . Note that these files are large and loading them can take a long time. Parameters Name Type Description Default language name of the language so that spaCy can grab correct tokenizer (example: \"en\" for english) required output_dir directory to save spaCy model required vectors_loc file containing the fasttext vectors None force with this flag raised we will always recreate the model from the vec.gz file False Usage : > lang = SpacyLanguage . from_fasttext ( \"nl\" , \"/path/spacy/model\" , \"~/Downloads/cc.nl.300.vec.gz\" ) > lang = SpacyLanguage . from_fasttext ( \"en\" , \"/path/spacy/model\" , \"~/Downloads/cc.en.300.vec.gz\" )","title":"from_fasttext()"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.score_similar","text":"Show source code in language/_spacy_lang.py 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning , ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/tfhub/","text":"whatlies.language.TFHubLanguage \u00b6 This class provides the abitilty to load and use text-embedding models of Tensorflow Hub to retrieve Embedding s or EmbeddingSet s from them. A list of supported models is available here ; however, note that only those models which operate directly on raw text (i.e. don't require any pre-processing such as tokenization) are supported for the moment (e.g. models such as BERT or ALBERT are not supported). Further, the TF-Hub compatible models from other repositories (i.e. other than tfhub.dev ) are also supported. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Further, consider that this language model mainly supports TensorFlow 2.x models (i.e. TF2 SavedModel format); although, TensorFlow 1.x models might be supported to some extent as well (see hub.load documentation as well as model compatibility guide ). Parameters Name Type Description Default url str The url or local directory path of the model. required tags Optional[List[str]] A set of strings specifying the graph variant to use, if loading from a TF1 module. It is passed to hub.load function. None signature Optional[str] An optional signature of the model to use. None Usage : > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_tfhub_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import TFHubLanguage > lang = TFHubLanguage(\"https://tfhub.dev/google/nnlm-en-dim50/2\") > lang['today is a gift'] > lang = TFHubLanguage(\"https://tfhub.dev/google/nnlm-en-dim50/2\") > lang[['withdraw some money', 'take out cash', 'cash out funds']] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]]","title":"TFHub"},{"location":"api/language/tfhub/#whatlieslanguagetfhublanguage","text":"This class provides the abitilty to load and use text-embedding models of Tensorflow Hub to retrieve Embedding s or EmbeddingSet s from them. A list of supported models is available here ; however, note that only those models which operate directly on raw text (i.e. don't require any pre-processing such as tokenization) are supported for the moment (e.g. models such as BERT or ALBERT are not supported). Further, the TF-Hub compatible models from other repositories (i.e. other than tfhub.dev ) are also supported. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Further, consider that this language model mainly supports TensorFlow 2.x models (i.e. TF2 SavedModel format); although, TensorFlow 1.x models might be supported to some extent as well (see hub.load documentation as well as model compatibility guide ). Parameters Name Type Description Default url str The url or local directory path of the model. required tags Optional[List[str]] A set of strings specifying the graph variant to use, if loading from a TF1 module. It is passed to hub.load function. None signature Optional[str] An optional signature of the model to use. None Usage : > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]]","title":"whatlies.language.TFHubLanguage"},{"location":"api/language/transformers/","text":"whatlies.language.HFTransformersLanguage \u00b6 This language class can be used to load Hugging Face Transformer models and use them to obtain representation of input string(s) as Embedding or EmbeddingSet . Important To use this language class, either of TensorFlow or PyTorch should be installed. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[transformers] pip install whatlies[all] Parameters Name Type Description Default model_name_or_path str A string which is the name or identifier of a model from Hugging Face model repository , or is the path to a local directory which contains a pre-trained transformer model files. required **kwargs Any Additional key-value pair argument(s) which are passed to transformers.pipeline function. {} Usage : > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_hftransformers_lang.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: A single string or a list of strings Returns: An instance of [Embedding][whatlies.embedding.Embedding] (when `query` is a string) or [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] (when `query` is a list of strings). The embedding vector is computed as the sum of hidden-state representaions of tokens (excluding special tokens, e.g. [CLS]). **Usage** ```python > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage('bert-base-cased') > lang['today is a nice day'] > lang = HFTransformersLanguage('gpt2') > lang[['day and night', 'it is as clear as day', 'today the sky is clear']] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] A single string or a list of strings required Returns Type Description `` An instance of Embedding (when query is a string) or EmbeddingSet (when query is a list of strings). The embedding vector is computed as the sum of hidden-state representaions of tokens (excluding special tokens, e.g. [CLS]). Usage > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]]","title":"Huggingface"},{"location":"api/language/transformers/#whatlieslanguagehftransformerslanguage","text":"This language class can be used to load Hugging Face Transformer models and use them to obtain representation of input string(s) as Embedding or EmbeddingSet . Important To use this language class, either of TensorFlow or PyTorch should be installed. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[transformers] pip install whatlies[all] Parameters Name Type Description Default model_name_or_path str A string which is the name or identifier of a model from Hugging Face model repository , or is the path to a local directory which contains a pre-trained transformer model files. required **kwargs Any Additional key-value pair argument(s) which are passed to transformers.pipeline function. {} Usage : > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]]","title":"whatlies.language.HFTransformersLanguage"},{"location":"api/transformers/addrandom/","text":"whatlies.transformers.AddRandom \u00b6 This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' ) fit ( self , embset ) \u00b6 Show source code in transformers/_addrandom.py 38 39 40 def fit ( self , embset ): self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_addrandom.py 42 43 44 45 46 47 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () np . random . seed ( self . seed ) orig_dict = embset . embeddings . copy () new_dict = { f \"rand_ { k } \" : Embedding ( f \"rand_ { k } \" , np . random . normal ( 0 , self . sigma , X . shape [ 1 ]) ) for k in range ( self . n ) } return EmbeddingSet ({ ** orig_dict , ** new_dict }) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Add Random"},{"location":"api/transformers/addrandom/#whatliestransformersaddrandom","text":"This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' )","title":"whatlies.transformers.AddRandom"},{"location":"api/transformers/addrandom/#whatlies.transformers._addrandom.AddRandom.fit","text":"Show source code in transformers/_addrandom.py 38 39 40 def fit ( self , embset ): self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/addrandom/#whatlies.transformers._addrandom.AddRandom.transform","text":"Show source code in transformers/_addrandom.py 42 43 44 45 46 47 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () np . random . seed ( self . seed ) orig_dict = embset . embeddings . copy () new_dict = { f \"rand_ { k } \" : Embedding ( f \"rand_ { k } \" , np . random . normal ( 0 , self . sigma , X . shape [ 1 ]) ) for k in range ( self . n ) } return EmbeddingSet ({ ** orig_dict , ** new_dict }) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/ivis/","text":"whatlies.transformers.Ivis \u00b6 This transformer scales all the vectors in an EmbeddingSet by means of Ivis algorithm. We're using the implementation found here . Important This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[ivis] pip install whatlies[all] Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Ivis implementation {} Usage: from whatlies.language import GensimLanguage from whatlies.transformers import Ivis words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = GensimLanguage ( \"wordvectors.kv\" ) emb = lang [ words ] emb . transform ( Ivis ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) fit ( self , embset ) \u00b6 Show source code in transformers/_ivis.py 51 52 53 54 55 def fit ( self , embset ): names , X = embset . to_names_X () self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_ivis.py 57 58 59 60 61 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .ivis_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Ivis"},{"location":"api/transformers/ivis/#whatliestransformersivis","text":"This transformer scales all the vectors in an EmbeddingSet by means of Ivis algorithm. We're using the implementation found here . Important This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[ivis] pip install whatlies[all] Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Ivis implementation {} Usage: from whatlies.language import GensimLanguage from whatlies.transformers import Ivis words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = GensimLanguage ( \"wordvectors.kv\" ) emb = lang [ words ] emb . transform ( Ivis ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Ivis"},{"location":"api/transformers/ivis/#whatlies.transformers._ivis.Ivis.fit","text":"Show source code in transformers/_ivis.py 51 52 53 54 55 def fit ( self , embset ): names , X = embset . to_names_X () self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/ivis/#whatlies.transformers._ivis.Ivis.transform","text":"Show source code in transformers/_ivis.py 57 58 59 60 61 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .ivis_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/noise/","text":"whatlies.transformers.Noise \u00b6 This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 )) fit ( self , embset ) \u00b6 Show source code in transformers/_noise.py 42 43 44 def fit ( self , embset ): self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_noise.py 46 47 48 49 50 51 52 53 54 def transform ( self , embset ): names , X = embset . to_names_X () np . random . seed ( self . seed ) new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } \" , ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Noise"},{"location":"api/transformers/noise/#whatliestransformersnoise","text":"This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"whatlies.transformers.Noise"},{"location":"api/transformers/noise/#whatlies.transformers._noise.Noise.fit","text":"Show source code in transformers/_noise.py 42 43 44 def fit ( self , embset ): self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/noise/#whatlies.transformers._noise.Noise.transform","text":"Show source code in transformers/_noise.py 46 47 48 49 50 51 52 53 54 def transform ( self , embset ): names , X = embset . to_names_X () np . random . seed ( self . seed ) new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } \" , ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/opentsne/","text":"whatlies.transformers.OpenTsne \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation used open-tsne . Important OpenTSNE is a faster variant of TSNE but it only allows for <2 components. You may also notice that it is relatively slow. This unfortunately is a fact of TSNE. This embedding transformation might require you to manually install extra dependencies unless you installed via either; pip install whatlies[opentsne] pip install whatlies[all] Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the OpenTsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import OpenTsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( OpenTsne ( 2 )) . plot_interactive_matrix () fit ( self , embset ) \u00b6 Show source code in transformers/_opentsne.py 55 56 57 58 59 def fit ( self , embset ): names , X = embset . to_names_X () self . emb = self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_opentsne.py 61 62 63 64 65 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = np . array ( self . emb . transform ( X )) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .tsne_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"OpenTsne"},{"location":"api/transformers/opentsne/#whatliestransformersopentsne","text":"This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation used open-tsne . Important OpenTSNE is a faster variant of TSNE but it only allows for <2 components. You may also notice that it is relatively slow. This unfortunately is a fact of TSNE. This embedding transformation might require you to manually install extra dependencies unless you installed via either; pip install whatlies[opentsne] pip install whatlies[all] Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the OpenTsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import OpenTsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( OpenTsne ( 2 )) . plot_interactive_matrix ()","title":"whatlies.transformers.OpenTsne"},{"location":"api/transformers/opentsne/#whatlies.transformers._opentsne.OpenTsne.fit","text":"Show source code in transformers/_opentsne.py 55 56 57 58 59 def fit ( self , embset ): names , X = embset . to_names_X () self . emb = self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/opentsne/#whatlies.transformers._opentsne.OpenTsne.transform","text":"Show source code in transformers/_opentsne.py 61 62 63 64 65 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = np . array ( self . emb . transform ( X )) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .tsne_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/pca/","text":"whatlies.transformers.Pca \u00b6 This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) fit ( self , embset ) \u00b6 Show source code in transformers/_pca.py 42 43 44 45 46 def fit ( self , embset ): names , X = embset . to_names_X () self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_pca.py 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .pca_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Pca"},{"location":"api/transformers/pca/#whatliestransformerspca","text":"This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Pca"},{"location":"api/transformers/pca/#whatlies.transformers._pca.Pca.fit","text":"Show source code in transformers/_pca.py 42 43 44 45 46 def fit ( self , embset ): names , X = embset . to_names_X () self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/pca/#whatlies.transformers._pca.Pca.transform","text":"Show source code in transformers/_pca.py 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .pca_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/tsne/","text":"whatlies.transformers.Tsne \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation uses scikit-learn . Important TSNE does not allow you to train a transformation and re-use it. It must retrain every time it sees data. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Tsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Tsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Tsne ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) fit ( self , embset ) \u00b6 Show source code in transformers/_tsne.py 46 47 48 49 50 def fit ( self , embset ): names , X = embset . to_names_X () self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_tsne.py 52 53 54 55 56 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = self . tfm . fit_transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .tsne_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Tsne"},{"location":"api/transformers/tsne/#whatliestransformerstsne","text":"This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation uses scikit-learn . Important TSNE does not allow you to train a transformation and re-use it. It must retrain every time it sees data. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Tsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Tsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Tsne ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Tsne"},{"location":"api/transformers/tsne/#whatlies.transformers._tsne.Tsne.fit","text":"Show source code in transformers/_tsne.py 46 47 48 49 50 def fit ( self , embset ): names , X = embset . to_names_X () self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/tsne/#whatlies.transformers._tsne.Tsne.transform","text":"Show source code in transformers/_tsne.py 52 53 54 55 56 def transform ( self , embset ): names , X = embset . to_names_X () new_vecs = self . tfm . fit_transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .tsne_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/umap/","text":"whatlies.transformers.Umap \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) fit ( self , embset ) \u00b6 Show source code in transformers/_umap.py 44 45 46 47 48 49 50 51 def fit ( self , embset ): names , X = embset . to_names_X () with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = UserWarning ) warnings . simplefilter ( \"ignore\" , category = NumbaPerformanceWarning ) self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_umap.py 53 54 55 56 57 58 59 def transform ( self , embset ): names , X = embset . to_names_X () with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = NumbaPerformanceWarning ) new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .umap_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Umap"},{"location":"api/transformers/umap/#whatliestransformersumap","text":"This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Umap"},{"location":"api/transformers/umap/#whatlies.transformers._umap.Umap.fit","text":"Show source code in transformers/_umap.py 44 45 46 47 48 49 50 51 def fit ( self , embset ): names , X = embset . to_names_X () with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = UserWarning ) warnings . simplefilter ( \"ignore\" , category = NumbaPerformanceWarning ) self . tfm . fit ( X ) self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/umap/#whatlies.transformers._umap.Umap.transform","text":"Show source code in transformers/_umap.py 53 54 55 56 57 58 59 def transform ( self , embset ): names , X = embset . to_names_X () with warnings . catch_warnings (): warnings . simplefilter ( \"ignore\" , category = NumbaPerformanceWarning ) new_vecs = self . tfm . transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .umap_ { self . n_components } ()\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"examples/lipstick-pig/","text":"In this example we'd like to discuss the effectiveness of debiasing techniques in word embeddings. This example is heavily inspired by the \"Lipstick on a Pig\" paper by Hila Gonen and Yoav Goldberg. Meaning \u00b6 In word embedding space you might wonder if a direction in embedding space might represent meaning. \\(v_{man}\\) represents the word embedding for \"man\" . \\(v_{woman}\\) represents the word embedding for \"woman\" . You could argue that the axis \\(v_{man} - v_{woman}\\) might represent \"gender\". One side of the \"gender\" axis would represent the male gender while the other end represents the female gender. This spectrum might allow you to guess if words are more \"male\" or more \"female\". It can also be used as a measurement for bias in language. Similarity \u00b6 There are some axes that we might come up with that should be similar to this gender axis, like: \\(v_{he} - v_{she}\\) \\(v_{king} - v_{queen}\\) \\(v_{brother} - v_{sister}\\) It would be highly unfortunate though if the following pairs of words would display similarity: \\(v_{nurse} - v_{physician}\\) \\(v_{nurse} - v_{surgeon}\\) \\(v_{nurse} - v_{doctor}\\) Being a nurse should not imply that you are a woman just like that being a surgeon should not imply that you are a man. It'd be a shame if we were using embeddings where such stereotypical bias is present. Unfortunately, it's likely in the embeddings. Historically, women have not gotten the same opportunities as men. This is bound to be reflected on websites like Wikipedia which are a common source of data to train word embeddings. So let's make a distance chart to confirm if this is the case. Word Pairs stereotype_pairs = [ ( 'sewing' , 'carpentry' ), ( 'nurse' , 'physician' ), ( 'nurse' , 'surgeon' ), ( 'nurse' , 'doctor' ), ] appropriate_pairs = [ ( 'woman' , 'man' ), ( 'she' , 'he' ), ( 'her' , 'him' ), ( 'girl' , 'boy' ) ] random_pairs = [ ( 'dog' , 'firehydrant' ), ( 'carpet' , 'leg' ), ( 'hot' , 'cold' ), ] all_pairs = [ stereotype_pairs , appropriate_pairs , random_pairs ] from whatlies import Embedding , EmbeddingSet from whatlies.language import FasttextLanguage lang_ft = FasttextLanguage ( \"cc.en.300.bin\" ) flatten = lambda l : [ item for sublist in l for item in sublist ] def calc_axis ( pair_list , language_model ): return [ language_model [ t1 ] - language_model [ t2 ] for ( t1 , t2 ) in pair_list ] def make_correlation_plot ( pairs , language_model , metric = \"cosine\" ): axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) emb_pairs . plot_distance ( metric = metric ) make_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) This code generates a similarity chart for fasttext embeddings, shown below. Notice, that we indeed see correlation. The \"gender\" direction seems to correlate with the \"doctor-nurse\" direction. We'd prefer if it simply were zero. Projections \u00b6 We observe bias that we do not want to have. So it's natural to ask: can we remove it? There is a popular technique that proposes to filter out the \"gender\"-direction. If we can quantify the gender direction then we might also be able to project all the vectors in our set away from it. The 2D plot below demonstrates this idea. Plot Code from whatlies import Embedding import matplotlib.pylab as plt man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); In this example you can see that if we project \\(v_{man}\\) away from the \\(v_{queen} - v_{king}\\) axis we get a new vector \\(v_{man} | (v_{queen} - v_{king})\\) . The 2D example also demonstrates that we might achieve: \\[v_{man} | (v_{queen} - v_{king}) \\approx v_{woman} | (v_{queen} - v_{king})\\] This suggests that we can use linear algebra to \"filter\" away the gender information as well as the gender bias. Post-Processing \u00b6 def make_debias_correlation_plot ( pairs , language_model , metric = 'cosine' ): # Calculate the embeddings just like before. axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) # Calculate the \"gender\"-direction norm_emb = EmbeddingSet ( ( language_model [ 'man' ] - language_model [ 'woman' ]), ( language_model [ 'king' ] - language_model [ 'queen' ]), ( language_model [ 'father' ] - language_model [ 'mother' ]) ) . average () # Project all embeddings away from this axis. emb_pairs = emb_pairs | norm_emb # Plot the result. emb_pairs . plot_distance ( metric = metric ) make_debias_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) We'll now display the \"before\" as well as \"after\" chart. It's not a perfect removal of the similarity. But we can confirm that at least visually, it seems \"less\". Across Languages \u00b6 One benefit of this library is that it is fairly easy to repeat this exercise for different language backends. Just replace the language_model with a different backend. Here's the results for three backends; a large English spaCy model, FastText and a large English BytePair model. Relative Distances \u00b6 The results look promising but we need to be very careful here. We're able to show that on one bias-metric we're performing better now. But we should not assume that this solves all issues related to gender in word embeddings. To demonstrate why, let's try and use a debiased space to predict gender using standard algorithms in scikit-learn. As a data source we'll take two gendered word-lists. You can download the same word-lists here and here . These wordlists are subsets of the wordlists used in the Learning Gender - Neutral Word Embeddings paper. The original, and larger, datasets can be found here . import pathlib from whatlies.transformers import Pca , Umap from whatlies.language import SpacyLanguage , FasttextLanguage male_word = pathlib . Path ( \"male-words.txt\" ) . read_text () . split ( \" \\n \" ) female_word = pathlib . Path ( \"female-words.txt\" ) . read_text () . split ( \" \\n \" ) lang = FasttextLanguage ( \"cc.en.300.bin\" ) e1 = lang [ male_word ] . add_property ( \"group\" , lambda d : \"male\" ) e2 = lang [ female_word ] . add_property ( \"group\" , lambda d : \"female\" ) emb_debias = e1 . merge ( e2 ) | ( lang [ 'man' ] - lang [ 'woman' ]) Next, we'll use the fasttext language backend as a scikit-learn featurizer. You can read more on this feature here . from sklearn.svm import SVC from sklearn.pipeline import Pipeline # There is overlap in the word-lists which we remove via `set`. words = list ( male_word ) + list ( female_word ) words = list ( set ( words )) labels = [ w in male_word for w in words ] # We use our language backend as a transformer in scikit-learn. pipe = Pipeline ([ ( \"embed\" , lang ), ( \"model\" , SVC ()) ]) This pipeline can now be used to make predictions. Currently we do not perform any debiasing, so let's have a look at how well we can predict gender now. Method I: Biased Embedding, Biased Model \u00b6 The code below runs the schematic drawn above. from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.metrics import classification_report X_train , X_test , y_train , y_test = train_test_split ( words , labels , train_size = 200 , random_state = 42 ) y_pred = pipe . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) This gives us the following result: precision recall f1-score support False 0.87 0.92 0.90 93 True 0.94 0.89 0.91 116 accuracy 0.90 209 macro avg 0.90 0.91 0.90 209 weighted avg 0.91 0.90 0.90 209 It seems that the information that is in the embeddings now give us a 90% accuracy on our test set. Method II: UnBiased Embedding, Biased Model \u00b6 If we now apply debiasing on the vectors then one might expect the old model to no longer be able to predict the gender. X , y = emb_debias . to_X_y ( 'group' ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 200 , random_state = 42 ) y_pred = pipe . steps [ 1 ][ 1 ] . predict ( X_test ) print ( classification_report ( y_pred , y_test == 'male' )) This gives the following result: precision recall f1-score support False 0.97 0.73 0.83 131 True 0.68 0.96 0.79 78 accuracy 0.81 209 macro avg 0.82 0.84 0.81 209 weighted avg 0.86 0.81 0.82 209 We're using the same model as before, but now we're giving it the debiased vectors to predict on. Despite being trained on a different dataset, we're still able to predict 81% of the cases accurately. This does not bode well for our debiasing technique. Method III: UnBiased Embedding, UnBiased Model \u00b6 We can also try to create a model that is both trained and applied on the unbiased vectors. y_pred = SVC () . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) precision recall f1-score support female 0.80 0.83 0.81 94 male 0.86 0.83 0.84 115 accuracy 0.83 209 macro avg 0.83 0.83 0.83 209 weighted avg 0.83 0.83 0.83 209 If we train a model on the debiased embeddings and also apply it to another debiased set we're able to get 83% of the cases right. We were hoping more around 50% here. Conclusion \u00b6 Based on just cosine distance it seems that we're able to remove the gender \"direction\" from our embeddings by using linear projections as a debiasing technique. However, if we use the debiased embeddings to predict gender it seems that we still have a reasonable amount of predictive power. This demonstrates that the debiasing technique has a limited effect and that there's plenty of reasons to remain careful and critical when applying word embeddings in practice.","title":"Debiasing Projections"},{"location":"examples/lipstick-pig/#meaning","text":"In word embedding space you might wonder if a direction in embedding space might represent meaning. \\(v_{man}\\) represents the word embedding for \"man\" . \\(v_{woman}\\) represents the word embedding for \"woman\" . You could argue that the axis \\(v_{man} - v_{woman}\\) might represent \"gender\". One side of the \"gender\" axis would represent the male gender while the other end represents the female gender. This spectrum might allow you to guess if words are more \"male\" or more \"female\". It can also be used as a measurement for bias in language.","title":"Meaning"},{"location":"examples/lipstick-pig/#similarity","text":"There are some axes that we might come up with that should be similar to this gender axis, like: \\(v_{he} - v_{she}\\) \\(v_{king} - v_{queen}\\) \\(v_{brother} - v_{sister}\\) It would be highly unfortunate though if the following pairs of words would display similarity: \\(v_{nurse} - v_{physician}\\) \\(v_{nurse} - v_{surgeon}\\) \\(v_{nurse} - v_{doctor}\\) Being a nurse should not imply that you are a woman just like that being a surgeon should not imply that you are a man. It'd be a shame if we were using embeddings where such stereotypical bias is present. Unfortunately, it's likely in the embeddings. Historically, women have not gotten the same opportunities as men. This is bound to be reflected on websites like Wikipedia which are a common source of data to train word embeddings. So let's make a distance chart to confirm if this is the case. Word Pairs stereotype_pairs = [ ( 'sewing' , 'carpentry' ), ( 'nurse' , 'physician' ), ( 'nurse' , 'surgeon' ), ( 'nurse' , 'doctor' ), ] appropriate_pairs = [ ( 'woman' , 'man' ), ( 'she' , 'he' ), ( 'her' , 'him' ), ( 'girl' , 'boy' ) ] random_pairs = [ ( 'dog' , 'firehydrant' ), ( 'carpet' , 'leg' ), ( 'hot' , 'cold' ), ] all_pairs = [ stereotype_pairs , appropriate_pairs , random_pairs ] from whatlies import Embedding , EmbeddingSet from whatlies.language import FasttextLanguage lang_ft = FasttextLanguage ( \"cc.en.300.bin\" ) flatten = lambda l : [ item for sublist in l for item in sublist ] def calc_axis ( pair_list , language_model ): return [ language_model [ t1 ] - language_model [ t2 ] for ( t1 , t2 ) in pair_list ] def make_correlation_plot ( pairs , language_model , metric = \"cosine\" ): axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) emb_pairs . plot_distance ( metric = metric ) make_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) This code generates a similarity chart for fasttext embeddings, shown below. Notice, that we indeed see correlation. The \"gender\" direction seems to correlate with the \"doctor-nurse\" direction. We'd prefer if it simply were zero.","title":"Similarity"},{"location":"examples/lipstick-pig/#projections","text":"We observe bias that we do not want to have. So it's natural to ask: can we remove it? There is a popular technique that proposes to filter out the \"gender\"-direction. If we can quantify the gender direction then we might also be able to project all the vectors in our set away from it. The 2D plot below demonstrates this idea. Plot Code from whatlies import Embedding import matplotlib.pylab as plt man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); In this example you can see that if we project \\(v_{man}\\) away from the \\(v_{queen} - v_{king}\\) axis we get a new vector \\(v_{man} | (v_{queen} - v_{king})\\) . The 2D example also demonstrates that we might achieve: \\[v_{man} | (v_{queen} - v_{king}) \\approx v_{woman} | (v_{queen} - v_{king})\\] This suggests that we can use linear algebra to \"filter\" away the gender information as well as the gender bias.","title":"Projections"},{"location":"examples/lipstick-pig/#post-processing","text":"def make_debias_correlation_plot ( pairs , language_model , metric = 'cosine' ): # Calculate the embeddings just like before. axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) # Calculate the \"gender\"-direction norm_emb = EmbeddingSet ( ( language_model [ 'man' ] - language_model [ 'woman' ]), ( language_model [ 'king' ] - language_model [ 'queen' ]), ( language_model [ 'father' ] - language_model [ 'mother' ]) ) . average () # Project all embeddings away from this axis. emb_pairs = emb_pairs | norm_emb # Plot the result. emb_pairs . plot_distance ( metric = metric ) make_debias_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) We'll now display the \"before\" as well as \"after\" chart. It's not a perfect removal of the similarity. But we can confirm that at least visually, it seems \"less\".","title":"Post-Processing"},{"location":"examples/lipstick-pig/#across-languages","text":"One benefit of this library is that it is fairly easy to repeat this exercise for different language backends. Just replace the language_model with a different backend. Here's the results for three backends; a large English spaCy model, FastText and a large English BytePair model.","title":"Across Languages"},{"location":"examples/lipstick-pig/#relative-distances","text":"The results look promising but we need to be very careful here. We're able to show that on one bias-metric we're performing better now. But we should not assume that this solves all issues related to gender in word embeddings. To demonstrate why, let's try and use a debiased space to predict gender using standard algorithms in scikit-learn. As a data source we'll take two gendered word-lists. You can download the same word-lists here and here . These wordlists are subsets of the wordlists used in the Learning Gender - Neutral Word Embeddings paper. The original, and larger, datasets can be found here . import pathlib from whatlies.transformers import Pca , Umap from whatlies.language import SpacyLanguage , FasttextLanguage male_word = pathlib . Path ( \"male-words.txt\" ) . read_text () . split ( \" \\n \" ) female_word = pathlib . Path ( \"female-words.txt\" ) . read_text () . split ( \" \\n \" ) lang = FasttextLanguage ( \"cc.en.300.bin\" ) e1 = lang [ male_word ] . add_property ( \"group\" , lambda d : \"male\" ) e2 = lang [ female_word ] . add_property ( \"group\" , lambda d : \"female\" ) emb_debias = e1 . merge ( e2 ) | ( lang [ 'man' ] - lang [ 'woman' ]) Next, we'll use the fasttext language backend as a scikit-learn featurizer. You can read more on this feature here . from sklearn.svm import SVC from sklearn.pipeline import Pipeline # There is overlap in the word-lists which we remove via `set`. words = list ( male_word ) + list ( female_word ) words = list ( set ( words )) labels = [ w in male_word for w in words ] # We use our language backend as a transformer in scikit-learn. pipe = Pipeline ([ ( \"embed\" , lang ), ( \"model\" , SVC ()) ]) This pipeline can now be used to make predictions. Currently we do not perform any debiasing, so let's have a look at how well we can predict gender now.","title":"Relative Distances"},{"location":"examples/lipstick-pig/#method-i-biased-embedding-biased-model","text":"The code below runs the schematic drawn above. from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.metrics import classification_report X_train , X_test , y_train , y_test = train_test_split ( words , labels , train_size = 200 , random_state = 42 ) y_pred = pipe . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) This gives us the following result: precision recall f1-score support False 0.87 0.92 0.90 93 True 0.94 0.89 0.91 116 accuracy 0.90 209 macro avg 0.90 0.91 0.90 209 weighted avg 0.91 0.90 0.90 209 It seems that the information that is in the embeddings now give us a 90% accuracy on our test set.","title":"Method I: Biased Embedding, Biased Model"},{"location":"examples/lipstick-pig/#method-ii-unbiased-embedding-biased-model","text":"If we now apply debiasing on the vectors then one might expect the old model to no longer be able to predict the gender. X , y = emb_debias . to_X_y ( 'group' ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 200 , random_state = 42 ) y_pred = pipe . steps [ 1 ][ 1 ] . predict ( X_test ) print ( classification_report ( y_pred , y_test == 'male' )) This gives the following result: precision recall f1-score support False 0.97 0.73 0.83 131 True 0.68 0.96 0.79 78 accuracy 0.81 209 macro avg 0.82 0.84 0.81 209 weighted avg 0.86 0.81 0.82 209 We're using the same model as before, but now we're giving it the debiased vectors to predict on. Despite being trained on a different dataset, we're still able to predict 81% of the cases accurately. This does not bode well for our debiasing technique.","title":"Method II: UnBiased Embedding, Biased Model"},{"location":"examples/lipstick-pig/#method-iii-unbiased-embedding-unbiased-model","text":"We can also try to create a model that is both trained and applied on the unbiased vectors. y_pred = SVC () . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) precision recall f1-score support female 0.80 0.83 0.81 94 male 0.86 0.83 0.84 115 accuracy 0.83 209 macro avg 0.83 0.83 0.83 209 weighted avg 0.83 0.83 0.83 209 If we train a model on the debiased embeddings and also apply it to another debiased set we're able to get 83% of the cases right. We were hoping more around 50% here.","title":"Method III: UnBiased Embedding, UnBiased Model"},{"location":"examples/lipstick-pig/#conclusion","text":"Based on just cosine distance it seems that we're able to remove the gender \"direction\" from our embeddings by using linear projections as a debiasing technique. However, if we use the debiased embeddings to predict gender it seems that we still have a reasonable amount of predictive power. This demonstrates that the debiasing technique has a limited effect and that there's plenty of reasons to remain careful and critical when applying word embeddings in practice.","title":"Conclusion"},{"location":"tutorial/scikit-learn/","text":"Scikit-Learn \u00b6 Many of the language-backends inside of this package can be used in scikit-learn pipelines. We've implemented a compatible .fit() and .transform() API which means that you could write scikit-learn pipelines like this: import numpy as np from whatlies.language import SpacyLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) This pipeline is using the embeddings from spaCy now and passing those to the logistic regression. pipe.predict_proba(X) # array([[0.37862409, 0.62137591], # [0.27858304, 0.72141696], # [0.21386529, 0.78613471], # [0.7155662 , 0.2844338 ], # [0.64924579, 0.35075421], # [0.76414156, 0.23585844]]) You could make a pipeline that generates both dense and sparse features by using a FeatureUnion . from sklearn.pipeline import FeatureUnion from sklearn.feature_extraction.text import CountVectorizer preprocess = FeatureUnion ([ ( \"dense\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"sparse\" , CountVectorizer ()) ]) Supported Models \u00b6 The following language backends can be used as scikit-learn feature transformations. whatlies.language.SpacyLanguage whatlies.language.FasttextLanguage whatlies.language.CountVectorLanguage whatlies.language.BytePairLanguage whatlies.language.ConveRTLanguage whatlies.language.GensimLanguage whatlies.language.HFTransformersLanguage whatlies.language.TFHubLanguage Caveats \u00b6 There's a few caveats to be aware of though. Fasttext as well as spaCy cannot be directly pickled so that means that you won't be able to save a pipeline if there's a whatlies component in it. This also means that you cannot use a gridsearch. Where possible we try to test against scikit-learn's testing utilities but for now the usecases should assume that you cannot use GridSearchCV and that you cannot pickle to disk. If you see a way to properly support this in general, let us know on github by creating an issue .","title":"Scikit-Learn Compatibility"},{"location":"tutorial/scikit-learn/#scikit-learn","text":"Many of the language-backends inside of this package can be used in scikit-learn pipelines. We've implemented a compatible .fit() and .transform() API which means that you could write scikit-learn pipelines like this: import numpy as np from whatlies.language import SpacyLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) This pipeline is using the embeddings from spaCy now and passing those to the logistic regression. pipe.predict_proba(X) # array([[0.37862409, 0.62137591], # [0.27858304, 0.72141696], # [0.21386529, 0.78613471], # [0.7155662 , 0.2844338 ], # [0.64924579, 0.35075421], # [0.76414156, 0.23585844]]) You could make a pipeline that generates both dense and sparse features by using a FeatureUnion . from sklearn.pipeline import FeatureUnion from sklearn.feature_extraction.text import CountVectorizer preprocess = FeatureUnion ([ ( \"dense\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"sparse\" , CountVectorizer ()) ])","title":"Scikit-Learn"},{"location":"tutorial/scikit-learn/#supported-models","text":"The following language backends can be used as scikit-learn feature transformations. whatlies.language.SpacyLanguage whatlies.language.FasttextLanguage whatlies.language.CountVectorLanguage whatlies.language.BytePairLanguage whatlies.language.ConveRTLanguage whatlies.language.GensimLanguage whatlies.language.HFTransformersLanguage whatlies.language.TFHubLanguage","title":"Supported Models"},{"location":"tutorial/scikit-learn/#caveats","text":"There's a few caveats to be aware of though. Fasttext as well as spaCy cannot be directly pickled so that means that you won't be able to save a pipeline if there's a whatlies component in it. This also means that you cannot use a gridsearch. Where possible we try to test against scikit-learn's testing utilities but for now the usecases should assume that you cannot use GridSearchCV and that you cannot pickle to disk. If you see a way to properly support this in general, let us know on github by creating an issue .","title":"Caveats"},{"location":"tutorial/embeddings/","text":"Imaginary Tokens \u00b6 Let's make a few word-embeddings. The basic object for this is an Embedding object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" ); Meaning \u00b6 Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' ); King - Man + Woman \u00b6 We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' ); King - Queen \u00b6 But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Man | (Queen - King) \u00b6 But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Embedding Mathmatics \u00b6 This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ] More Operations \u00b6 Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Mapping Unto Tokens \u00b6 In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Measuring the Mapping \u00b6 Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522 Interesting \u00b6 This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( -. 3 , 0.4 ) plt . ylim ( -. 3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769 Plotting High Dimensions \u00b6 Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier ways to deal with collections of embeddings.","title":"What Are Embeddings"},{"location":"tutorial/embeddings/#imaginary-tokens","text":"Let's make a few word-embeddings. The basic object for this is an Embedding object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" );","title":"Imaginary Tokens"},{"location":"tutorial/embeddings/#meaning","text":"Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' );","title":"Meaning"},{"location":"tutorial/embeddings/#king-man-woman","text":"We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' );","title":"King - Man + Woman"},{"location":"tutorial/embeddings/#king-queen","text":"But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"King - Queen"},{"location":"tutorial/embeddings/#man-queen-king","text":"But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"Man | (Queen - King)"},{"location":"tutorial/embeddings/#embedding-mathmatics","text":"This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ]","title":"Embedding Mathmatics"},{"location":"tutorial/embeddings/#more-operations","text":"Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"More Operations"},{"location":"tutorial/embeddings/#mapping-unto-tokens","text":"In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"Mapping Unto Tokens"},{"location":"tutorial/embeddings/#measuring-the-mapping","text":"Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522","title":"Measuring the Mapping"},{"location":"tutorial/embeddings/#interesting","text":"This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( -. 3 , 0.4 ) plt . ylim ( -. 3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769","title":"Interesting"},{"location":"tutorial/embeddings/#plotting-high-dimensions","text":"Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier ways to deal with collections of embeddings.","title":"Plotting High Dimensions"},{"location":"tutorial/embeddingsets/","text":"Sets of Embeddings \u00b6 The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an Direct Creation \u00b6 You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings. Via Languages \u00b6 But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] Plotting \u00b6 Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); Combining Charts \u00b6 Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web. More Transformation \u00b6 But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive () plot_umap = umap_emb . plot_interactive () plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err }); Operators \u00b6 Note that the operators that we've seen before can also be added to a transformation pipeline. emb . transform ( lambda e : e | ( e [ \"man\" ] - e [ \"woman\" ])) # (Emb | (Emb[man] - Emb[woman])).pca_2() More Components \u00b6 Suppose now that we'd like to visualise three principal components. We could do this. pca_emb = emb . transform ( Pca ( 3 )) p1 = pca_emb . plot_interactive () p2 = pca_emb . plot_interactive ( 2 , 1 ) p1 | p2 fetch('tut2-chart5.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis5', out); }) .catch(err => { throw err }); More Charts \u00b6 Let's not draw two components at a time, let's draw all of them. pca_emb . plot_interactive_matrix ( 0 , 1 , 2 ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"Interactive Visualisation"},{"location":"tutorial/embeddingsets/#sets-of-embeddings","text":"The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an","title":"Sets of Embeddings"},{"location":"tutorial/embeddingsets/#direct-creation","text":"You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings.","title":"Direct Creation"},{"location":"tutorial/embeddingsets/#via-languages","text":"But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"bluee\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ]","title":"Via Languages"},{"location":"tutorial/embeddingsets/#plotting","text":"Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err });","title":"Plotting"},{"location":"tutorial/embeddingsets/#combining-charts","text":"Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web.","title":"Combining Charts"},{"location":"tutorial/embeddingsets/#more-transformation","text":"But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive () plot_umap = umap_emb . plot_interactive () plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"More Transformation"},{"location":"tutorial/embeddingsets/#operators","text":"Note that the operators that we've seen before can also be added to a transformation pipeline. emb . transform ( lambda e : e | ( e [ \"man\" ] - e [ \"woman\" ])) # (Emb | (Emb[man] - Emb[woman])).pca_2()","title":"Operators"},{"location":"tutorial/embeddingsets/#more-components","text":"Suppose now that we'd like to visualise three principal components. We could do this. pca_emb = emb . transform ( Pca ( 3 )) p1 = pca_emb . plot_interactive () p2 = pca_emb . plot_interactive ( 2 , 1 ) p1 | p2 fetch('tut2-chart5.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis5', out); }) .catch(err => { throw err });","title":"More Components"},{"location":"tutorial/embeddingsets/#more-charts","text":"Let's not draw two components at a time, let's draw all of them. pca_emb . plot_interactive_matrix ( 0 , 1 , 2 ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"More Charts"},{"location":"tutorial/languages/","text":"In this tool we have support for different language backends and depending on the language backend you may get slightly different behavior. Multiple Tokens \u00b6 We can have spaCy summerize multiple tokens if we'd like. from whatlies.language.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) contexts = [ \"i am super duper happy\" , \"happy happy joy joy\" , \"programming is super fun!\" , \"i am going crazy i hate it\" , \"boo and hiss\" ,] emb = lang [ contexts ] emb . transform ( Pca ( 2 )) . plot_interactive () fetch('spacyvec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); Under the hood it will be calculating the averages of the embeddings but we can still plot these. Bert Style \u00b6 But spaCy also offers transformers these days, which means that we can play with a extra bit of context. pip install spacy-transformers python -m spacy download en_trf_robertabase_lg With these installed we can now use the same spaCy language backend to play with transformers. Here's an example of two embeddings selected with context. lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) np . array_equal ( lang [ 'Going to the [store]' ] . vector , lang [ '[store] this in the drawer please.' ] . vector ) # False In the first case we get the embedding for store in the context of Going to the store while in the second case we have store in the context of store this in the drawer please . Sense to Vec \u00b6 We also have support for the sense2vec model . To use them you'll need to make sure that you install via: pip install whatlies[sense2vec] Once the dependency is installed you can download and unzip the pretrained vectors found here . After that you should be able to retreive tokens with context. They way you fetch these tokens is a bit ... different though. from whatlies.language.language import Sense2VecLanguage from whatlies.transformers import Pca lang = Sense2VecLanguage ( \"path/downloaded/s2v\" ) words = [ \"bank|NOUN\" , \"bank|VERB\" , \"duck|NOUN\" , \"duck|VERB\" , \"dog|NOUN\" , \"cat|NOUN\" , \"jump|VERB\" , \"run|VERB\" , \"chicken|NOUN\" , \"puppy|NOUN\" , \"kitten|NOUN\" , \"carrot|NOUN\" ] emb = lang [ words ] You're able to distinguish the verb \"duck\" from the animal \"duck\". From here one we're back to normal embeddingsets though. So we can plot whatever we feel like. p1 = emb . plot_interactive ( \"dog|NOUN\" , \"jump|VERB\" ) p2 = emb . transform ( Pca ( 2 )) . plot_interactive () p1 | p2 fetch('sense2vec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#s1', out); }) .catch(err => { throw err }); Notice how duck|VERB is certainly different from duck|NOUN . Similarity \u00b6 Another nice feature of sense2vec is the ability to find tokens that are nearby. We could do the following. lang . score_similar ( \"duck|VERB\" ) This will result in a long list with embedding-score tuples. [(Emb[crouch|VERB], 0.8064), (Emb[ducking|VERB], 0.7877), (Emb[sprint|VERB], 0.7653), (Emb[scoot|VERB], 0.7647), (Emb[dart|VERB], 0.7621), (Emb[jump|VERB], 0.7528), (Emb[peek|VERB], 0.7518), (Emb[ducked|VERB], 0.7504), (Emb[bonk|VERB], 0.7495), (Emb[backflip|VERB], 0.746)] We can also ask it to return an EmbeddingSet instead. That's what we're doing below. We take our original embeddingset and we merge it with two more before we visualise it. emb_bank_verb = lang . embset_similar ( \"bank|VERB\" , n = 10 ) emb_bank_noun = lang . embset_similar ( \"bank|NOUN\" , n = 10 ) ( emb . merge ( emb_bank_verb ) . merge ( emb_bank_noun ) . transform ( Pca ( 2 )) . plot_interactive ()) fetch('sense2vec-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#sense2', out); }) .catch(err => { throw err });","title":"Language Options"},{"location":"tutorial/languages/#multiple-tokens","text":"We can have spaCy summerize multiple tokens if we'd like. from whatlies.language.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) contexts = [ \"i am super duper happy\" , \"happy happy joy joy\" , \"programming is super fun!\" , \"i am going crazy i hate it\" , \"boo and hiss\" ,] emb = lang [ contexts ] emb . transform ( Pca ( 2 )) . plot_interactive () fetch('spacyvec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); Under the hood it will be calculating the averages of the embeddings but we can still plot these.","title":"Multiple Tokens"},{"location":"tutorial/languages/#bert-style","text":"But spaCy also offers transformers these days, which means that we can play with a extra bit of context. pip install spacy-transformers python -m spacy download en_trf_robertabase_lg With these installed we can now use the same spaCy language backend to play with transformers. Here's an example of two embeddings selected with context. lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) np . array_equal ( lang [ 'Going to the [store]' ] . vector , lang [ '[store] this in the drawer please.' ] . vector ) # False In the first case we get the embedding for store in the context of Going to the store while in the second case we have store in the context of store this in the drawer please .","title":"Bert Style"},{"location":"tutorial/languages/#sense-to-vec","text":"We also have support for the sense2vec model . To use them you'll need to make sure that you install via: pip install whatlies[sense2vec] Once the dependency is installed you can download and unzip the pretrained vectors found here . After that you should be able to retreive tokens with context. They way you fetch these tokens is a bit ... different though. from whatlies.language.language import Sense2VecLanguage from whatlies.transformers import Pca lang = Sense2VecLanguage ( \"path/downloaded/s2v\" ) words = [ \"bank|NOUN\" , \"bank|VERB\" , \"duck|NOUN\" , \"duck|VERB\" , \"dog|NOUN\" , \"cat|NOUN\" , \"jump|VERB\" , \"run|VERB\" , \"chicken|NOUN\" , \"puppy|NOUN\" , \"kitten|NOUN\" , \"carrot|NOUN\" ] emb = lang [ words ] You're able to distinguish the verb \"duck\" from the animal \"duck\". From here one we're back to normal embeddingsets though. So we can plot whatever we feel like. p1 = emb . plot_interactive ( \"dog|NOUN\" , \"jump|VERB\" ) p2 = emb . transform ( Pca ( 2 )) . plot_interactive () p1 | p2 fetch('sense2vec-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#s1', out); }) .catch(err => { throw err }); Notice how duck|VERB is certainly different from duck|NOUN .","title":"Sense to Vec"},{"location":"tutorial/languages/#similarity","text":"Another nice feature of sense2vec is the ability to find tokens that are nearby. We could do the following. lang . score_similar ( \"duck|VERB\" ) This will result in a long list with embedding-score tuples. [(Emb[crouch|VERB], 0.8064), (Emb[ducking|VERB], 0.7877), (Emb[sprint|VERB], 0.7653), (Emb[scoot|VERB], 0.7647), (Emb[dart|VERB], 0.7621), (Emb[jump|VERB], 0.7528), (Emb[peek|VERB], 0.7518), (Emb[ducked|VERB], 0.7504), (Emb[bonk|VERB], 0.7495), (Emb[backflip|VERB], 0.746)] We can also ask it to return an EmbeddingSet instead. That's what we're doing below. We take our original embeddingset and we merge it with two more before we visualise it. emb_bank_verb = lang . embset_similar ( \"bank|VERB\" , n = 10 ) emb_bank_noun = lang . embset_similar ( \"bank|NOUN\" , n = 10 ) ( emb . merge ( emb_bank_verb ) . merge ( emb_bank_noun ) . transform ( Pca ( 2 )) . plot_interactive ()) fetch('sense2vec-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#sense2', out); }) .catch(err => { throw err });","title":"Similarity"},{"location":"tutorial/observations/","text":"Observations \u00b6 This document will demonstrate some observations of embeddings. The goal of this document is to two-fold. It hopes to explain how to use whatlies . It hopes to explain interesting elements of different embeddings. Spelling Errors \u00b6 Especially when you're designing a chatbot, spelling errors occur all the time. So how do different embeddings deal with this? Let's compare three different language backends here. from whatlies.language import FasttextLanguage , SpacyLanguage , CountVectorLanguage lang_spacy = SpacyLanguage ( \"en_core_web_md\" ) lang_fasttext = FasttextLanguage ( \"cc.en.300.bin\" ) lang_cv = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 3 ), analyzer = \"char\" ) lang_cv . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' , 'cat' , 'dog' , 'pikachu' , 'pokemon' ]) Besides fetching a fasttext and spaCy model you'll notice that we're also manually fitting the CountVectorLanguage . Take a note of this because this fact will be meaningful later. words = [ 'piza' , 'pizza' , 'pizzza' , 'italian' , 'sushi' , 'japan' , 'burger' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' , 'house' , 'tree' , 'elephant' , 'pikachu' , 'pokemon' ] def mk_plot ( lang ): return ( lang [ words ] . transform ( Pca ( 2 )) . plot_interactive () . properties ( height = 250 , width = 250 , title = lang . __class__ . __name__ )) ( mk_plot ( lang_spacy ) & mk_plot ( lang_fasttext ) & mk_plot ( lang_cv )) fetch('chart-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); You may notice a few things here. The CountVectorLanguage only looks at character similarities which it is why it feels that \"piza\", \"pizza\" and \"pizzza\" are all very similar. Notice how spaCy disagrees with this. You may also note that fasttext is \"in between\". That is because these embeddings also encode the subtokens. The fasttext embeddings seem to do a better job at catching certain forms of meaning. It understands that pikachu, pokemon and japan are related while also associating burger/sushi with pizza. The spaCy model also captures this but the clustering is less appearant. This can be due to the dimensionality reduction though. You'll notice that the two clearly misspelled words, fyrehouse and fyrehidrant, are mapped to the same point in the spaCy embedding. When you check the embeddings for both you'll confirm why. SpaCy maps a token to a vector of zeros is it is not available in the vocabulary. SpaCy may occasionally also map two different tokens to the same embedding in an attempt to save on disk space. Fasttext is able to recover more context because of the subtoken embeddings but also because the embeddings are way bigger . The fasttext embeddings unzipped can be 7GB on disk while spaCy model is on 115MB on disk. Notice how the CountVectorLanguage has a cluster with lots of things in it. It clusters together \"house\", \"elephant\", \"sushi\" and \"pokemon\". This isn't because of shared meaning. It because these words contain combinations of characters that weren't there in the training set that we gave to the fit_manual method in the beginning. Note that \"picachu\" is similar to \"pizza\" for a similar reason.","title":"Observations"},{"location":"tutorial/observations/#observations","text":"This document will demonstrate some observations of embeddings. The goal of this document is to two-fold. It hopes to explain how to use whatlies . It hopes to explain interesting elements of different embeddings.","title":"Observations"},{"location":"tutorial/observations/#spelling-errors","text":"Especially when you're designing a chatbot, spelling errors occur all the time. So how do different embeddings deal with this? Let's compare three different language backends here. from whatlies.language import FasttextLanguage , SpacyLanguage , CountVectorLanguage lang_spacy = SpacyLanguage ( \"en_core_web_md\" ) lang_fasttext = FasttextLanguage ( \"cc.en.300.bin\" ) lang_cv = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 3 ), analyzer = \"char\" ) lang_cv . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' , 'cat' , 'dog' , 'pikachu' , 'pokemon' ]) Besides fetching a fasttext and spaCy model you'll notice that we're also manually fitting the CountVectorLanguage . Take a note of this because this fact will be meaningful later. words = [ 'piza' , 'pizza' , 'pizzza' , 'italian' , 'sushi' , 'japan' , 'burger' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' , 'house' , 'tree' , 'elephant' , 'pikachu' , 'pokemon' ] def mk_plot ( lang ): return ( lang [ words ] . transform ( Pca ( 2 )) . plot_interactive () . properties ( height = 250 , width = 250 , title = lang . __class__ . __name__ )) ( mk_plot ( lang_spacy ) & mk_plot ( lang_fasttext ) & mk_plot ( lang_cv )) fetch('chart-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#c1', out); }) .catch(err => { throw err }); You may notice a few things here. The CountVectorLanguage only looks at character similarities which it is why it feels that \"piza\", \"pizza\" and \"pizzza\" are all very similar. Notice how spaCy disagrees with this. You may also note that fasttext is \"in between\". That is because these embeddings also encode the subtokens. The fasttext embeddings seem to do a better job at catching certain forms of meaning. It understands that pikachu, pokemon and japan are related while also associating burger/sushi with pizza. The spaCy model also captures this but the clustering is less appearant. This can be due to the dimensionality reduction though. You'll notice that the two clearly misspelled words, fyrehouse and fyrehidrant, are mapped to the same point in the spaCy embedding. When you check the embeddings for both you'll confirm why. SpaCy maps a token to a vector of zeros is it is not available in the vocabulary. SpaCy may occasionally also map two different tokens to the same embedding in an attempt to save on disk space. Fasttext is able to recover more context because of the subtoken embeddings but also because the embeddings are way bigger . The fasttext embeddings unzipped can be 7GB on disk while spaCy model is on 115MB on disk. Notice how the CountVectorLanguage has a cluster with lots of things in it. It clusters together \"house\", \"elephant\", \"sushi\" and \"pokemon\". This isn't because of shared meaning. It because these words contain combinations of characters that weren't there in the training set that we gave to the fit_manual method in the beginning. Note that \"picachu\" is similar to \"pizza\" for a similar reason.","title":"Spelling Errors"},{"location":"tutorial/transformations/","text":"State and Colors \u00b6 A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); Visualising Differences \u00b6 Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize. Movement \u00b6 If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Transformation Viz"},{"location":"tutorial/transformations/#state-and-colors","text":"A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err });","title":"State and Colors"},{"location":"tutorial/transformations/#visualising-differences","text":"Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize.","title":"Visualising Differences"},{"location":"tutorial/transformations/#movement","text":"If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Movement"}]}